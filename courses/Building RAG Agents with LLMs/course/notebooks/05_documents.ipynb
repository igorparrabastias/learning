{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e",
   "metadata": {
    "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b4e8b-269c-4cc8-8470-1db4a91b6c34",
   "metadata": {
    "id": "1c3b4e8b-269c-4cc8-8470-1db4a91b6c34"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 5:** Working with Large Documents</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "In the previous notebook, we learned about running state chains and knowledge bases! By the end, we had all the tools necessary to do some simple dialog management and custom knowledge tracking. In this notebook, we will take the same ideas and move towards the space of large documents, considering what kinds of issues we will run into as we try to incorporate large files into our LLM contexts.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Get familiar with document loaders and the kinds of utilities they might provide you.\n",
    "- Learn how to parse large documents with limited context room by chunking the document and building up a knowledge base progressively.\n",
    "- Understand how the progressive recontextualization, coersion, and consolidation of document chunks can be extremely useful, and also where it will encounter natural limitations.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- Looking at the chunks that come out of your ArxivParser, you'll notice that some of the chunks make little sense on their own or have been completely corrupted by the conversion to text. Is it doing a pass over the chunks to clean them up?\n",
    "- Considering the document summarization workflow (or any similar workflow that processes through a large list of document chunks), how often should this happen, and when is it justifiable?\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Notebook Source:**\n",
    "\n",
    "- This notebook is part of a larger [**NVIDIA Deep Learning Institute**](https://www.nvidia.com/en-us/training/) course titled [**Building RAG Agents with LLMs**](https://www.nvidia.com/en-sg/training/instructor-led-workshops/building-rag-agents-with-llms/). If sharing this material, please give credit and link back to the original course.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Environment Setup:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9214bd93-d65d-4dbd-94e3-254a2f670c52",
   "metadata": {
    "id": "9214bd93-d65d-4dbd-94e3-254a2f670c52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -qq langchain langchain-nvidia-ai-endpoints gradio\n",
    "# %pip install -qq arxiv pymupdf\n",
    "%pip install -qq langchain_openai\n",
    "\n",
    "# import os\n",
    "# os.environ[\"NVIDIA_API_KEY\"] = \"nvapi-...\"\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de8583a1-c10a-41da-8256-49520f868670",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful utility method for printing intermediate states\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from functools import partial\n",
    "\n",
    "def RPrint(preface=\"State: \"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        print(f\"{preface}{x}\")\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def PPrint(preface=\"State: \"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        pprint(preface, x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 1:** Chatting with Documents\n",
    "\n",
    "This notebook will begin a longer stream of discussion surrounding the use of LLMs to chat with documents. In a world where chat models are trained on giant repositories of public data and retraining them on custom data is prohibitively expensive, the idea of having an LLM reason about a set of PDFs or even a YouTube video opens up many opportunities!\n",
    "\n",
    "- **Your LLM can have a modifiable knowledge base grounded in human-readible documents,** meaning that you can directly control what kinds of data it has access to and can instruct it to interact with it.\n",
    "\n",
    "- **Your LLM can sort through and pull references directly from your document set.** With sufficient prompt engineering and instruction-following priors, you can force your models to only act based on the material you provide.\n",
    "\n",
    "- **Your LLM can possibly even interact with your documents, making automatic modifications as necessary.** This opens up avenues in automatic content refinement and synthetic operations which will be explored later.\n",
    "\n",
    "Listing out some possibilities is pretty easy, and from there you can let your imagination run wild... but we haven't gained the tools to do this quite yet, right?\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Naive Approach: Stuff Your Document**\n",
    "\n",
    "Suppose you have some text documents (PDF, blog, etc.) and want to ask questions related to the contents of those documents. One approach you could try involves taking a representation of the document and feeding it all to a chat model! From a document perspective, this is known as [**document stuffing**](https://python.langchain.com/docs/modules/chains/document/stuff).\n",
    "\n",
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=14DRI_uDviqzqg14TKoIc8IlBc3Zsb8oO\" width=800px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/doc_stuff.png\" width=800px/>\n",
    ">\n",
    "> From [**Stuff | LangChain**ü¶úÔ∏èüîó](https://python.langchain.com/docs/modules/chains/document/stuff)\n",
    "\n",
    "<br>\n",
    "\n",
    "This may very well work if your model is strong enough and if your document is short enough, but it shouldn't be expected to work well for an entire document. Many modern LLMs have significant trouble working with long contexts due to training limitations. Nowadays large model deterioration isn't quite as catastrophic, but good instruction following is likely to fall apart pretty quickly regardless of which model you use (assuming you're accessing the raw model).\n",
    "\n",
    "<br>\n",
    "\n",
    "**The key issues you'll need to resolve with document reasoning are:**\n",
    "\n",
    "- How do we split our documents into pieces that can be reasoned with?\n",
    "\n",
    "- How can we find and consider these pieces efficiently as the size and number of documents increases?\n",
    "\n",
    "This course will explore several approaches to address these issues while continuing to build up our LLM orchestration skills. ***This notebook will serve to expand out our previous running chain skills for more progressive reasoning formulations, whereas the next notebooks will introduce some new techniques to properly address retrieval at scale.*** Through this experience, we will continue to leverage cutting-edge open-source solutions to make our solutions standard and integratable.\n",
    "\n",
    "Speaking of, the field of document loading frameworks has many strong options, and two major players will come up throughout the course:\n",
    "\n",
    "- [**LangChain**](https://python.langchain.com/docs/get_started/introduction) provides a simple framework for connecting LLMs to your own data sources via general chunking strategies and strong incorporation with embedding frameworks/services. This framework has initially grown around its strong general support for LLM features, which signals its active strengths closer to the chain abstractions and agent coordination.\n",
    "\n",
    "- [**LlamaIndex**](https://gpt-index.readthedocs.io/en/stable/) is a data framework for LLM applications to ingest, structure, and access private or domain-specific data. It has since branched out to include general LLM capabilities similar to LangChain, but as of now it is still strongest in addressing the document side of LLM components since its initial abstractions were centered around that problem.\n",
    "\n",
    "It's recommended to read more about the unique strengths of both LlamaIndex and LangChain and pick the one that works best for you. Since LlamaIndex can be used *with* LangChain, the frameworks' unique capabilities [can be leveraged together without too much issue](https://docs.llamaindex.ai/en/stable/community/integrations/using_with_langchain.html). For the sake of simplicity, we will stick to LangChain in this course and will allow the [**NVIDIA/GenerativeAIExamples repository**](https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RetrievalAugmentedGeneration/notebooks) to explore deeper LlamaIndex options for those interested.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310462b-f215-4d00-9d59-e613921bed0a",
   "metadata": {
    "id": "3310462b-f215-4d00-9d59-e613921bed0a"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** Loading Documents\n",
    "\n",
    "LangChain provides a variety of [document loaders](https://python.langchain.com/docs/integrations/document_loaders) to facilitate the injestion of various document formats (HTML, PDF, code) from many different sources and locations (local storage, private s3 buckets, public websites, messaging APIs, etc.). These loaders query your data sources and return a `Document` object which contains the content and metadata, usually in a plain-text or otherwise human-readible format. There are plenty of document loaders already built and ready to use, with the first-party LangChain options listed [here](https://python.langchain.com/docs/integrations/document_loaders).\n",
    "\n",
    "**In this example, we can load a research paper of our choice using one of the following LangChain loaders:**\n",
    "- [`UnstructuredFileLoader`](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file): Generally-useful file loader for arbitrary files; doesn't make too many assumptions about your document structure and is usually sufficient.\n",
    "- [`ArxivLoader`](https://python.langchain.com/docs/integrations/document_loaders/arxiv): A more specialized file-loader which can communicate with the Arxiv interface directly. [Just one example of many](https://python.langchain.com/docs/integrations/document_loaders), this will make some more assumptions about your data to yield nicer parsings and auto-fill metadata (useful when you have multiple documents/formats).\n",
    "\n",
    "For our code example we will default to using `ArxivLoader` to load in one of either the [MRKL](https://arxiv.org/abs/2205.00445) or [ReAct](https://arxiv.org/abs/2210.03629) publication papers as you're likely to run into them at some point in your continued chat model research endeavors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4382b61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3944,
     "status": "ok",
     "timestamp": 1703112979370,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "b4382b61",
    "outputId": "d6e95b9b-97be-4984-a9fd-58a528091146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.04 s, sys: 119 ms, total: 1.15 s\n",
      "Wall time: 2.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "## Loading in the file\n",
    "\n",
    "## Unstructured File Loader: Good for arbitrary \"probably good enough\" loader\n",
    "# documents = UnstructuredFileLoader(\"llama2_paper.pdf\").load()\n",
    "\n",
    "## More specialized loader, won't work for everything, but simple API and usually better results\n",
    "documents = ArxivLoader(query=\"2404.16130\").load()  ## GraphRAG\n",
    "# documents = ArxivLoader(query=\"2404.03622\").load()  ## Visualization-of-Thought\n",
    "# documents = ArxivLoader(query=\"2404.19756\").load()  ## KAN: Kolmogorov-Arnold Networks\n",
    "# documents = ArxivLoader(query=\"2404.07143\").load()  ## Infini-Attention\n",
    "# documents = ArxivLoader(query=\"2210.03629\").load()  ## ReAct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hw0SL--6cirp",
   "metadata": {
    "id": "hw0SL--6cirp"
   },
   "source": [
    "<br>\n",
    "\n",
    "We can see from our import that we this connector gives us access to two different components:\n",
    "- The `page_content` is the actual body of the document in some human-interpretable format.\n",
    "- The `metadata` is relevant information about the document that is provided by the connector via its data source.\n",
    "\n",
    "Below, we can check out the length of our document body to see what's inside, and will probably notice an intractable document length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2289d525-2c2b-4a99-9a48-00f9b951ae02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 370,
     "status": "ok",
     "timestamp": 1703113455184,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "2289d525-2c2b-4a99-9a48-00f9b951ae02",
    "outputId": "98b9ef68-c36b-478f-9bbb-1e45b2c49d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents Retrieved: 1\n",
      "Sample of Document 1 Content (Total Length: 53880):\n",
      "From Local to Global: A Graph RAG Approach to\n",
      "Query-Focused Summarization\n",
      "Darren Edge1‚Ä†\n",
      "Ha Trinh1‚Ä†\n",
      "Newman Cheng2\n",
      "Joshua Bradley2\n",
      "Alex Chao3\n",
      "Apurva Mody3\n",
      "Steven Truitt2\n",
      "Jonathan Larson1\n",
      "1Microsoft Research\n",
      "2Microsoft Strategic Missions and Technologies\n",
      "3Microsoft Office of the CTO\n",
      "{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,steventruitt,jolarso}\n",
      "@microsoft.com\n",
      "‚Ä†These authors contributed equally to this work\n",
      "Abstract\n",
      "The use of retrieval-augmented generation (RAG) to retrieve relevant informa-\n",
      "tion from an external knowledge source enables large language models (LLMs)\n",
      "to answer questions over private and/or previously unseen document collections.\n",
      "However, RAG fails on global questions directed at an entire text corpus, such\n",
      "as ‚ÄúWhat are the main themes in the dataset?‚Äù, since this is inherently a query-\n",
      "focused summarization (QFS) task, rather than an explicit retrieval task. Prior\n",
      "QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical\n",
      "RAG systems. \n"
     ]
    }
   ],
   "source": [
    "## Printing out a sample of the content\n",
    "print(\"Number of Documents Retrieved:\", len(documents))\n",
    "print(f\"Sample of Document 1 Content (Total Length: {len(documents[0].page_content)}):\")\n",
    "print(documents[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1JjUK2ZSd0HL",
   "metadata": {
    "id": "1JjUK2ZSd0HL"
   },
   "source": [
    "<br>\n",
    "\n",
    "In contrast, the metadata will be much more conservatively-sized to the point of being viable context components for your favorite chat model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "Py2lbRXlcX81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1703112982386,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "Py2lbRXlcX81",
    "outputId": "07197dd4-1609-4ecf-ae54-cf6ef3d25458"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2024-04-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'From Local to Global: A Graph RAG Approach to Query-Focused Summarization'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Jonathan Larson'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'The use of retrieval-augmented generation (RAG) to retrieve relevant\\ninformation from an external </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge source enables large language models\\n(LLMs) to answer questions over private and/or previously unseen </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">document\\ncollections. However, RAG fails on global questions directed at an entire text\\ncorpus, such as \"What are</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the main themes in the dataset?\", since this is\\ninherently a query-focused summarization (QFS) task, rather than </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an explicit\\nretrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities\\nof text indexed by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">typical RAG systems. To combine the strengths of these\\ncontrasting methods, we propose a Graph RAG approach to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">question answering over\\nprivate text corpora that scales with both the generality of user questions and\\nthe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">quantity of source text to be indexed. Our approach uses an LLM to build a\\ngraph-based text index in two stages: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">first to derive an entity knowledge graph\\nfrom the source documents, then to pregenerate community summaries for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">all\\ngroups of closely-related entities. Given a question, each community summary is\\nused to generate a partial </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">response, before all partial responses are again\\nsummarized in a final response to the user. For a class of global</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sensemaking\\nquestions over datasets in the 1 million token range, we show that Graph RAG\\nleads to substantial </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">improvements over a na\\\\\"ive RAG baseline for both the\\ncomprehensiveness and diversity of generated answers. An </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">open-source,\\nPython-based implementation of both global and local Graph RAG approaches is\\nforthcoming at </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://aka.ms/graphrag.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2024-04-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'From Local to Global: A Graph RAG Approach to Query-Focused Summarization'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, \u001b[0m\n",
       "\u001b[32mJonathan Larson'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'The use of retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to retrieve relevant\\ninformation from an external \u001b[0m\n",
       "\u001b[32mknowledge source enables large language models\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to answer questions over private and/or previously unseen \u001b[0m\n",
       "\u001b[32mdocument\\ncollections. However, RAG fails on global questions directed at an entire text\\ncorpus, such as \"What are\u001b[0m\n",
       "\u001b[32mthe main themes in the dataset?\", since this is\\ninherently a query-focused summarization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m task, rather than \u001b[0m\n",
       "\u001b[32man explicit\\nretrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities\\nof text indexed by \u001b[0m\n",
       "\u001b[32mtypical RAG systems. To combine the strengths of these\\ncontrasting methods, we propose a Graph RAG approach to \u001b[0m\n",
       "\u001b[32mquestion answering over\\nprivate text corpora that scales with both the generality of user questions and\\nthe \u001b[0m\n",
       "\u001b[32mquantity of source text to be indexed. Our approach uses an LLM to build a\\ngraph-based text index in two stages: \u001b[0m\n",
       "\u001b[32mfirst to derive an entity knowledge graph\\nfrom the source documents, then to pregenerate community summaries for \u001b[0m\n",
       "\u001b[32mall\\ngroups of closely-related entities. Given a question, each community summary is\\nused to generate a partial \u001b[0m\n",
       "\u001b[32mresponse, before all partial responses are again\\nsummarized in a final response to the user. For a class of global\u001b[0m\n",
       "\u001b[32msensemaking\\nquestions over datasets in the 1 million token range, we show that Graph RAG\\nleads to substantial \u001b[0m\n",
       "\u001b[32mimprovements over a na\\\\\"ive RAG baseline for both the\\ncomprehensiveness and diversity of generated answers. An \u001b[0m\n",
       "\u001b[32mopen-source,\\nPython-based implementation of both global and local Graph RAG approaches is\\nforthcoming at \u001b[0m\n",
       "\u001b[32mhttps://aka.ms/graphrag.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046ea74-0b81-400e-8364-449f421d2add",
   "metadata": {
    "id": "7046ea74-0b81-400e-8364-449f421d2add"
   },
   "source": [
    "<br>\n",
    "\n",
    "Though it may be tempting to accept the metadata format as-is and ignore the body entirely, there are a key selection of features that cannot be approached without diving into the full text:\n",
    "\n",
    "- **The metadata is not guaranteed.** In the case of `arxiv`, paper abstracts, titles, authors, and date are necessary components of a submission, so being able to query them is not surprising. For an arbitrary PDF or webpage though, the same is not necessarily the case.\n",
    "- **The agent will not be able to go deeper into the document content.** The summary is good to know and can be used as-is, but does not provide a straight-forward path to interacting with the body at any capacity (at least not from what we've learned).\n",
    "- **The agent will still not be able to reason about too many documents at once.** Perhaps in the MRKL/ReAct example, you could combine those two summaries into one context and ask some questions. But what happens when you need to interact with 5 documents at once? What about an entire directory? Very soon, you will notice that your context window will be overloaded with information just to summarize or even list out the documents you're interested in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0449e4",
   "metadata": {
    "id": "4e0449e4"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3:** Transforming The Documents\n",
    "\n",
    "Once documents have been loaded, they often need to be transformed if we intend to pass them into our LLMs as context. One method of transformation is known as **chunking**, which breaks down large pieces of content into smaller segments. This technique is valuable because it helps [optimize the relevance of the content returned from the vector database](https://www.pinecone.io/learn/chunking-strategies/).\n",
    "\n",
    "LangChain provides a [variety of document transformers](https://python.langchain.com/docs/integrations/document_transformers/) out of which we will use the [``RecursiveCharacterTextSplitter``](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter). This option will allow us tp split our document with preference for some natural stopping points that we want our chunks to follow (as much as possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f564ee4-262e-4721-bf6b-ee8ebdb7a1ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1703112527056,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "6f564ee4-262e-4721-bf6b-ee8ebdb7a1ba",
    "outputId": "a4e666e5-5a5c-413b-f5a4-acca742d80d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "## Some nice custom preprocessing\n",
    "# documents[0].page_content = documents[0].page_content.replace(\". .\", \"\")\n",
    "docs_split = text_splitter.split_documents(documents)\n",
    "\n",
    "# def include_doc(doc):\n",
    "#     ## Some chunks will be overburdened with useless numerical data, so we'll filter it out\n",
    "#     string = doc.page_content\n",
    "#     if len([l for l in string if l.isalpha()]) < (len(string)//2):\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# docs_split = [doc for doc in docs_split if include_doc(doc)]\n",
    "print(len(docs_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f8bcc89-c781-44d0-9ec1-1fe45eec8b46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1703112530925,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "1f8bcc89-c781-44d0-9ec1-1fe45eec8b46",
    "outputId": "1cf24605-65bb-40a2-e7aa-e2d9a8fb6382"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Local to Global: A Graph RAG Approach to\n",
      "Query-Focused Summarization\n",
      "Darren Edge1‚Ä†\n",
      "Ha Trinh1‚Ä†\n",
      "Newman Cheng2\n",
      "Joshua Bradley2\n",
      "Alex Chao3\n",
      "Apurva Mody3\n",
      "Steven Truitt2\n",
      "Jonathan Larson1\n",
      "1Microsoft Research\n",
      "2Microsoft Strategic Missions and Technologies\n",
      "3Microsoft Office of the CTO\n",
      "{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,steventruitt,jolarso}\n",
      "@microsoft.com\n",
      "‚Ä†These authors contributed equally to this work\n",
      "Abstract\n",
      "The use of retrieval-augmented generation (RAG) to retrieve relevant informa-\n",
      "tion from an external knowledge source enables large language models (LLMs)\n",
      "to answer questions over private and/or previously unseen document collections.\n",
      "However, RAG fails on global questions directed at an entire text corpus, such\n",
      "as ‚ÄúWhat are the main themes in the dataset?‚Äù, since this is inherently a query-\n",
      "focused summarization (QFS) task, rather than an explicit retrieval task. Prior\n",
      "QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical\n",
      "RAG systems. To combine the strengths of these contrasting methods, we propose\n",
      "a Graph RAG approach to question answering over private text corpora that scales\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a Graph RAG approach to question answering over private text corpora that scales\n",
      "with both the generality of user questions and the quantity of source text to be in-\n",
      "dexed. Our approach uses an LLM to build a graph-based text index in two stages:\n",
      "first to derive an entity knowledge graph from the source documents, then to pre-\n",
      "generate community summaries for all groups of closely-related entities. Given a\n",
      "question, each community summary is used to generate a partial response, before\n",
      "all partial responses are again summarized in a final response to the user. For a\n",
      "class of global sensemaking questions over datasets in the 1 million token range,\n",
      "we show that Graph RAG leads to substantial improvements over a na¬®\n",
      "ƒ±ve RAG\n",
      "baseline for both the comprehensiveness and diversity of generated answers. An\n",
      "open-source, Python-based implementation of both global and local Graph RAG\n",
      "approaches is forthcoming at https://aka.ms/graphrag.\n",
      "1\n",
      "Introduction\n",
      "Human endeavors across a range of domains rely on our ability to read and reason about large\n",
      "collections of documents, often reaching conclusions that go beyond anything stated in the source\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections of documents, often reaching conclusions that go beyond anything stated in the source\n",
      "texts themselves. With the emergence of large language models (LLMs), we are already witnessing\n",
      "attempts to automate human-like sensemaking in complex domains like scientific discovery (Mi-\n",
      "crosoft, 2023) and intelligence analysis (Ranade and Joshi, 2023), where sensemaking is defined as\n",
      "Preprint. Under review.\n",
      "arXiv:2404.16130v1  [cs.CL]  24 Apr 2024\n",
      "Source Documents\n",
      "Text Chunks\n",
      "text extraction\n",
      "and chunking\n",
      "Element Instances\n",
      "domain-tailored\n",
      "summarization\n",
      "Element Summaries\n",
      "domain-tailored\n",
      "summarization\n",
      "Graph Communities\n",
      "community\n",
      "detection\n",
      "Community Summaries\n",
      "domain-tailored\n",
      "summarization\n",
      "Community Answers\n",
      "query-focused\n",
      "summarization\n",
      "Global Answer\n",
      "query-focused\n",
      "summarization\n",
      "Indexing Time\n",
      "Query Time\n",
      "Pipeline Stage\n",
      "Figure 1: Graph RAG pipeline using an LLM-derived graph index of source document text. This\n",
      "index spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims) that have\n",
      "been detected, extracted, and summarized by LLM prompts tailored to the domain of the dataset.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6\n",
      "Community Summaries ‚ÜíCommunity Answers ‚ÜíGlobal Answer\n",
      "Given a user query, the community summaries generated in the previous step can be used to generate\n",
      "a final answer in a multi-stage process. The hierarchical nature of the community structure also\n",
      "means that questions can be answered using the community summaries from different levels, raising\n",
      "the question of whether a particular level in the hierarchical community structure offers the best\n",
      "balance of summary detail and scope for general sensemaking questions (evaluated in section 3).\n",
      "For a given community level, the global answer to any user query is generated as follows:\n",
      "‚Ä¢ Prepare community summaries. Community summaries are randomly shuffled and divided\n",
      "into chunks of pre-specified token size. This ensures relevant information is distributed\n",
      "across chunks, rather than concentrated (and potentially lost) in a single context window.\n",
      "‚Ä¢ Map community answers. Generate intermediate answers in parallel, one for each chunk.\n",
      "The LLM is also asked to generate a score between 0-100 indicating how helpful the gen-\n",
      "erated answer is in answering the target question. Answers with score 0 are filtered out.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m-1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on\n",
      "Empirical Methods in Natural Language Processing (EMNLP).\n",
      "Yao, J.-g., Wan, X., and Xiao, J. (2017). Recent advances in document summarization. Knowledge\n",
      "and Information Systems, 53:297‚Äì336.\n",
      "14\n",
      "Yao, L., Peng, J., Mao, C., and Luo, Y. (2023). Exploring large language models for knowledge\n",
      "graph completion.\n",
      "Zhang, J. (2023). Graph-toolformer: To empower llms with graph reasoning ability via prompt\n",
      "augmented by chatgpt. arXiv preprint arXiv:2304.11116.\n",
      "Zhang, Y., Zhang, Y., Gan, Y., Yao, L., and Wang, C. (2024). Causal graph discovery with retrieval-\n",
      "augmented generation based large language models. arXiv preprint arXiv:2402.15301.\n",
      "Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing,\n",
      "E., et al. (2024). Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural\n",
      "Information Processing Systems, 36.\n",
      "15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in (0, 1, 2, 15, -1):\n",
    "    pprint(f\"[Document {i}]\")\n",
    "    print(docs_split[i].page_content)\n",
    "    pprint(\"=\"*64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e2f969-72cd-4d0e-a150-e3efafc1cdfc",
   "metadata": {
    "id": "57e2f969-72cd-4d0e-a150-e3efafc1cdfc"
   },
   "source": [
    "<br>\n",
    "\n",
    "Our approach for chunking is pretty naive, but highlights the ease of getting at least something working for our application. We made some effort to keep the chunk size small so that our models are able to wield it effectively as context, but how are we going to reason about all of these pieces?\n",
    "\n",
    "**When extending and optimizing this approach for an arbitrary set of documents, some potential options include:**\n",
    "\n",
    "- Identifying logical breaks or synthesis techniques (manually, automatically, LLM-assisted, etc).\n",
    "- Aiming to construct chunks that are rich in unique and relevant information, avoiding redundancy to maximize database utility.\n",
    "- Customizing chunking to fit the document‚Äôs nature, ensuring the chunks are contextually relevant and cohesive.\n",
    "- Including key concepts, keywords, or metadata snippets in each chunk for improved searchability and relevance in the database.\n",
    "- Continuously assessing chunking effectiveness and be ready to adjust strategies for optimal balance between size and content richness.\n",
    "- Considering a hierarchy system (implicitly-generated or explicitly-specified) to improve retrieval attempts.\n",
    "    - If interested, please look over the [**LlamaIndex tree structures from the index guide**](https://docs.llamaindex.ai/en/stable/module_guides/indexing/index_guide.html#tree-index) as a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-0QApYgNbyJD",
   "metadata": {
    "id": "-0QApYgNbyJD"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4: [Exercise]** Refining Summaries\n",
    "\n",
    "To automatically reason about large documents, one potential idea might be to use LLMs to create a dense summary or knowledge base. Similar to how we maintained a running history of the conversation via slot-filling in the previous notebook, is there any problem with keeping a running history of an entire document?\n",
    "\n",
    "In this section, we focus on an exciting application of LLMs: **automatically refining, coercing, and consolidating data en masse**. Specifically, we'll be implementing a simple but useful Runnable that uses a while loop and the running state chain formulation to summarize a set of document chunks. This process is commonly known as [**\"document refinement\"**](https://python.langchain.com/docs/modules/chains/document/refine) and is largely akin to our previous conversation-focused slot-filling exercise; the only difference is that now we're dealing with a large document instead of a growing chat history.\n",
    "\n",
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1J2XR8Cc8YSkVJMiJCknMkgA02mBT8riZ\" width=1000px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/doc_refine.png\" width=1000px/>\n",
    ">\n",
    "> From [**Refine | LangChain**ü¶úÔ∏èüîó](https://python.langchain.com/docs/modules/chains/document/refine)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **The DocumentSummaryBase Model**\n",
    "\n",
    "Much like the `KnowledgeBase` class from the previous notebook, we can create a `DocumentSummaryBase` structure designed to encapsulate the essence of a document. The one below will use the `running_summary` field to query the model for a final summary while attempting to use the `main_ideas` and `loose_ends` fields as a bottleneck to keep the running summary from moving too fast. This is something we're going to have to enforce via prompt engineering, so the `summary_prompt` is also provided which shows how this information will be used. Feel free to modify it as necessary to make it work for your model of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "gE8y2JvLvZ5T",
   "metadata": {
    "id": "gE8y2JvLvZ5T"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "class DocumentSummaryBase(BaseModel):\n",
    "    running_summary: str = Field(\n",
    "        \"\", description=\"Descripci√≥n en curso del documento. ¬°No lo sobrescribas; solo actualiza!\")\n",
    "    main_ideas: List[str] = Field(\n",
    "        [], description=\"Informaci√≥n m√°s importante del documento (m√°x. 3)\")\n",
    "    loose_ends: List[str] = Field(\n",
    "        [], description=\"Preguntas abiertas que ser√≠a bueno incorporar en el resumen, pero que a√∫n son desconocidas (m√°x. 3)\")\n",
    "\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Est√°s generando un resumen en curso del documento. Hazlo legible para un usuario t√©cnico.\"\n",
    "    \" Despu√©s de esto, la antigua base de conocimientos ser√° reemplazada por la nueva. Aseg√∫rate de que un lector a√∫n pueda entender todo.\"\n",
    "    \" Mantenlo corto, pero tan denso y √∫til como sea posible. La informaci√≥n debe fluir de fragmento a (preguntas abiertas o ideas principales) a resumen en curso.\"\n",
    "    \" La base de conocimientos actualizada mantiene toda la informaci√≥n del resumen en curso aqu√≠: {info_base}.\"\n",
    "    \"\\n\\n{format_instructions}. Sigue el formato con precisi√≥n, incluyendo citas y comas\"\n",
    "    \"\\n\\nSin perder ninguna de la informaci√≥n, actualiza la base de conocimientos con lo siguiente: {input}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7LkjfpOAvlEd",
   "metadata": {
    "id": "7LkjfpOAvlEd"
   },
   "source": [
    "<br>\n",
    "\n",
    "We will also use this opportunity to bring back the `RExtract` function from the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "khRhVghHxBaz",
   "metadata": {
    "id": "khRhVghHxBaz"
   },
   "outputs": [],
   "source": [
    "def RExtract(pydantic_class, llm, prompt):\n",
    "    '''\n",
    "    Runnable Extraction module\n",
    "    Returns a knowledge dictionary populated by slot-filling extraction\n",
    "    '''\n",
    "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
    "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
    "    def preparse(string):\n",
    "        if '{' not in string: string = '{' + string\n",
    "        if '}' not in string: string = string + '}'\n",
    "        string = (string\n",
    "            .replace(\"\\\\_\", \"_\")\n",
    "            .replace(\"\\n\", \" \")\n",
    "            .replace(\"\\]\", \"]\")\n",
    "            .replace(\"\\[\", \"[\")\n",
    "        )\n",
    "        # print(string)  ## Good for diagnostics\n",
    "        return string\n",
    "    return instruct_merge | prompt | llm | preparse | parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oFtME_s4PRoW",
   "metadata": {
    "id": "oFtME_s4PRoW"
   },
   "source": [
    "<br>\n",
    "\n",
    "With this in mind, the following code invokes the running state chain in a for-loop to iterate over your documents! The only modification necessary should be the `parse_chain` implementation, which should pass the the state through a properly-configured `RExtract` chain from the last notebook. After this, the system should work decently to maintain a running summary of the document (though some tweaking of the prompt may be required depending on the model used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acec30bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['OPENAI_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6sODIfHUgz6m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79192,
     "status": "ok",
     "timestamp": 1703112894722,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "6sODIfHUgz6m",
    "outputId": "7b5aee70-078b-458e-d2a7-e8601b789fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered 1 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'El documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation (RAG) al abordar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preguntas globales sobre colecciones de textos. Se destaca que el RAG es efectivo para recuperar informaci√≥n </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relevante, pero no para tareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. La </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">combinaci√≥n de m√©todos de QFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">y efectividad en la respuesta a preguntas sobre grandes vol√∫menes de texto no estructurado.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'Graph RAG combina recuperaci√≥n de informaci√≥n y generaci√≥n de res√∫menes enfocados en consultas.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'RAG es ineficaz para preguntas globales sobre un corpus completo.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'Los m√©todos de QFS anteriores no escalan adecuadamente con la cantidad de texto manejado por sistemas </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'¬øQu√© m√©tricas se utilizar√°n para evaluar la efectividad del enfoque Graph RAG?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'¬øC√≥mo se integrar√°n las fuentes de conocimiento externas en el nuevo sistema?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'¬øQu√© mejoras espec√≠ficas se esperan en comparaci√≥n con los m√©todos anteriores?'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'El documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en \u001b[0m\n",
       "\u001b[32mconsultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m al abordar \u001b[0m\n",
       "\u001b[32mpreguntas globales sobre colecciones de textos. Se destaca que el RAG es efectivo para recuperar informaci√≥n \u001b[0m\n",
       "\u001b[32mrelevante, pero no para tareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. La \u001b[0m\n",
       "\u001b[32mcombinaci√≥n de m√©todos de QFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad\u001b[0m\n",
       "\u001b[32my efectividad en la respuesta a preguntas sobre grandes vol√∫menes de texto no estructurado.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'Graph RAG combina recuperaci√≥n de informaci√≥n y generaci√≥n de res√∫menes enfocados en consultas.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'RAG es ineficaz para preguntas globales sobre un corpus completo.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'Los m√©todos de QFS anteriores no escalan adecuadamente con la cantidad de texto manejado por sistemas \u001b[0m\n",
       "\u001b[32mRAG.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'¬øQu√© m√©tricas se utilizar√°n para evaluar la efectividad del enfoque Graph RAG?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'¬øC√≥mo se integrar√°n las fuentes de conocimiento externas en el nuevo sistema?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'¬øQu√© mejoras espec√≠ficas se esperan en comparaci√≥n con los m√©todos anteriores?'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered 2 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'El documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation (RAG) al abordar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preguntas globales sobre colecciones de textos. Se destaca que el RAG es efectivo para recuperar informaci√≥n </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relevante, pero no para tareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. La </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">combinaci√≥n de m√©todos de QFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">y efectividad en la respuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">utiliza un LLM para construir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conocimiento de entidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. Este </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas generadas en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de datos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">aproximadamente 1 mill√≥n de tokens. Se anticipa una implementaci√≥n de c√≥digo abierto en Python de los enfoques </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Graph RAG disponibles en https://aka.ms/graphrag.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'Graph RAG combina recuperaci√≥n de informaci√≥n y generaci√≥n de res√∫menes enfocados en consultas.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'RAG es ineficaz para preguntas globales sobre un corpus completo.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'El enfoque Graph RAG mejora la exhaustividad y diversidad de respuestas en comparaci√≥n con un RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ingenuo.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'¬øQu√© m√©tricas se utilizar√°n para evaluar la efectividad del enfoque Graph RAG?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'¬øC√≥mo se integrar√°n las fuentes de conocimiento externas en el nuevo sistema?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'¬øQu√© mejoras espec√≠ficas se esperan en comparaci√≥n con los m√©todos anteriores?'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'El documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en \u001b[0m\n",
       "\u001b[32mconsultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m al abordar \u001b[0m\n",
       "\u001b[32mpreguntas globales sobre colecciones de textos. Se destaca que el RAG es efectivo para recuperar informaci√≥n \u001b[0m\n",
       "\u001b[32mrelevante, pero no para tareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. La \u001b[0m\n",
       "\u001b[32mcombinaci√≥n de m√©todos de QFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad\u001b[0m\n",
       "\u001b[32my efectividad en la respuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG \u001b[0m\n",
       "\u001b[32mutiliza un LLM para construir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de \u001b[0m\n",
       "\u001b[32mconocimiento de entidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. Este \u001b[0m\n",
       "\u001b[32mm√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas generadas en \u001b[0m\n",
       "\u001b[32mcomparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de datos de \u001b[0m\n",
       "\u001b[32maproximadamente 1 mill√≥n de tokens. Se anticipa una implementaci√≥n de c√≥digo abierto en Python de los enfoques \u001b[0m\n",
       "\u001b[32mGraph RAG disponibles en https://aka.ms/graphrag.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'Graph RAG combina recuperaci√≥n de informaci√≥n y generaci√≥n de res√∫menes enfocados en consultas.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'RAG es ineficaz para preguntas globales sobre un corpus completo.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'El enfoque Graph RAG mejora la exhaustividad y diversidad de respuestas en comparaci√≥n con un RAG \u001b[0m\n",
       "\u001b[32mingenuo.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'¬øQu√© m√©tricas se utilizar√°n para evaluar la efectividad del enfoque Graph RAG?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'¬øC√≥mo se integrar√°n las fuentes de conocimiento externas en el nuevo sistema?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'¬øQu√© mejoras espec√≠ficas se esperan en comparaci√≥n con los m√©todos anteriores?'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered 3 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'El documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation (RAG) al abordar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">los textos fuente. Con la aparici√≥n de modelos de lenguaje grandes (LLMs), ya estamos presenciando intentos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">automatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. La combinaci√≥n de m√©todos de QFS </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la respuesta</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para construir un</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">√≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de entidades y luego </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generando res√∫menes comunitarios para grupos de entidades relacionadas. Este m√©todo ha demostrado mejoras </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sustanciales en la exhaustividad y diversidad de las respuestas generadas en comparaci√≥n con un RAG ingenuo, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">especialmente para preguntas de sentido global sobre conjuntos de datos de aproximadamente 1 mill√≥n de tokens. El </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos (e.g., entidades), aristas (e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relaciones) y covariables (e.g., afirmaciones) que han sido detectadas, extra√≠das y resumidas mediante prompts de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de c√≥digo abierto en Python de los </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">enfoques Graph RAG disponibles en https://aka.ms/graphrag.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'Graph RAG combina recuperaci√≥n de informaci√≥n y generaci√≥n de res√∫menes enfocados en consultas.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'RAG es ineficaz para preguntas globales sobre un corpus completo.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'El enfoque Graph RAG mejora la exhaustividad y diversidad de respuestas en comparaci√≥n con un RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ingenuo.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'¬øQu√© m√©tricas se utilizar√°n para evaluar la efectividad del enfoque Graph RAG?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'¬øC√≥mo se integrar√°n las fuentes de conocimiento externas en el nuevo sistema?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'¬øQu√© mejoras espec√≠ficas se esperan en comparaci√≥n con los m√©todos anteriores?'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'El documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en \u001b[0m\n",
       "\u001b[32mconsultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m al abordar \u001b[0m\n",
       "\u001b[32mpreguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en\u001b[0m\n",
       "\u001b[32mlos textos fuente. Con la aparici√≥n de modelos de lenguaje grandes \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, ya estamos presenciando intentos de \u001b[0m\n",
       "\u001b[32mautomatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el \u001b[0m\n",
       "\u001b[32man√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para \u001b[0m\n",
       "\u001b[32mtareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. La combinaci√≥n de m√©todos de QFS \u001b[0m\n",
       "\u001b[32mprevios con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la respuesta\u001b[0m\n",
       "\u001b[32ma preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para construir un\u001b[0m\n",
       "\u001b[32m√≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de entidades y luego \u001b[0m\n",
       "\u001b[32mgenerando res√∫menes comunitarios para grupos de entidades relacionadas. Este m√©todo ha demostrado mejoras \u001b[0m\n",
       "\u001b[32msustanciales en la exhaustividad y diversidad de las respuestas generadas en comparaci√≥n con un RAG ingenuo, \u001b[0m\n",
       "\u001b[32mespecialmente para preguntas de sentido global sobre conjuntos de datos de aproximadamente 1 mill√≥n de tokens. El \u001b[0m\n",
       "\u001b[32mpipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., entidades\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, aristas \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \u001b[0m\n",
       "\u001b[32mrelaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m y covariables \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., afirmaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que han sido detectadas, extra√≠das y resumidas mediante prompts de \u001b[0m\n",
       "\u001b[32mLLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de c√≥digo abierto en Python de los \u001b[0m\n",
       "\u001b[32menfoques Graph RAG disponibles en https://aka.ms/graphrag.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'Graph RAG combina recuperaci√≥n de informaci√≥n y generaci√≥n de res√∫menes enfocados en consultas.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'RAG es ineficaz para preguntas globales sobre un corpus completo.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'El enfoque Graph RAG mejora la exhaustividad y diversidad de respuestas en comparaci√≥n con un RAG \u001b[0m\n",
       "\u001b[32mingenuo.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'¬øQu√© m√©tricas se utilizar√°n para evaluar la efectividad del enfoque Graph RAG?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'¬øC√≥mo se integrar√°n las fuentes de conocimiento externas en el nuevo sistema?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'¬øQu√© mejoras espec√≠ficas se esperan en comparaci√≥n con los m√©todos anteriores?'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered 4 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"El documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation (RAG) al abordar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">los textos fuente. Con la aparici√≥n de modelos de lenguaje grandes (LLMs), ya estamos presenciando intentos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">automatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. La combinaci√≥n de m√©todos de QFS </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la respuesta</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para construir un</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">√≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de entidades y luego </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de comunidades (e.g., Leiden, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Traag et al., 2019) se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos (nodos, aristas, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">covariables) que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de consulta. Este </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas generadas en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de datos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">aproximadamente 1 mill√≥n de tokens. La 'respuesta global' a una consulta dada se produce mediante una ronda final </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan relevancia para esa </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos (e.g., entidades), aristas </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(e.g., relaciones) y covariables (e.g., afirmaciones) que han sido detectadas, extra√≠das y resumidas mediante </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de c√≥digo abierto en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag.\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'Graph RAG combina recuperaci√≥n de informaci√≥n y generaci√≥n de res√∫menes enfocados en consultas.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'RAG es ineficaz para preguntas globales sobre un corpus completo.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'El enfoque Graph RAG mejora la exhaustividad y diversidad de respuestas en comparaci√≥n con un RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ingenuo.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'¬øQu√© m√©tricas se utilizar√°n para evaluar la efectividad del enfoque Graph RAG?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'¬øC√≥mo se integrar√°n las fuentes de conocimiento externas en el nuevo sistema?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'¬øQu√© mejoras espec√≠ficas se esperan en comparaci√≥n con los m√©todos anteriores?'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"El\u001b[0m\u001b[32m documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en \u001b[0m\n",
       "\u001b[32mconsultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m al abordar \u001b[0m\n",
       "\u001b[32mpreguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en\u001b[0m\n",
       "\u001b[32mlos textos fuente. Con la aparici√≥n de modelos de lenguaje grandes \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, ya estamos presenciando intentos de \u001b[0m\n",
       "\u001b[32mautomatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el \u001b[0m\n",
       "\u001b[32man√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para \u001b[0m\n",
       "\u001b[32mtareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. La combinaci√≥n de m√©todos de QFS \u001b[0m\n",
       "\u001b[32mprevios con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la respuesta\u001b[0m\n",
       "\u001b[32ma preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para construir un\u001b[0m\n",
       "\u001b[32m√≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de entidades y luego \u001b[0m\n",
       "\u001b[32mgenerando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de comunidades \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Leiden, \u001b[0m\n",
       "\u001b[32mTraag et al., 2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos \u001b[0m\u001b[32m(\u001b[0m\u001b[32mnodos, aristas, \u001b[0m\n",
       "\u001b[32mcovariables\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de consulta. Este \u001b[0m\n",
       "\u001b[32mm√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas generadas en \u001b[0m\n",
       "\u001b[32mcomparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de datos de \u001b[0m\n",
       "\u001b[32maproximadamente 1 mill√≥n de tokens. La 'respuesta global' a una consulta dada se produce mediante una ronda final \u001b[0m\n",
       "\u001b[32mde resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan relevancia para esa \u001b[0m\n",
       "\u001b[32mconsulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., entidades\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, aristas \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32me.g., relaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m y covariables \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., afirmaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que han sido detectadas, extra√≠das y resumidas mediante \u001b[0m\n",
       "\u001b[32mprompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de c√≥digo abierto en \u001b[0m\n",
       "\u001b[32mPython de los enfoques Graph RAG disponibles en https://aka.ms/graphrag.\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'Graph RAG combina recuperaci√≥n de informaci√≥n y generaci√≥n de res√∫menes enfocados en consultas.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'RAG es ineficaz para preguntas globales sobre un corpus completo.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'El enfoque Graph RAG mejora la exhaustividad y diversidad de respuestas en comparaci√≥n con un RAG \u001b[0m\n",
       "\u001b[32mingenuo.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'¬øQu√© m√©tricas se utilizar√°n para evaluar la efectividad del enfoque Graph RAG?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'¬øC√≥mo se integrar√°n las fuentes de conocimiento externas en el nuevo sistema?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'¬øQu√© mejoras espec√≠ficas se esperan en comparaci√≥n con los m√©todos anteriores?'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered 5 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"El documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation (RAG) al abordar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">los textos fuente. Con la aparici√≥n de modelos de lenguaje grandes (LLMs), ya estamos presenciando intentos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">automatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'resumici√≥n enfocada en consultas' (QFS, Dang, 2006), particularmente la 'resumici√≥n abstr√°ctica enfocada en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">QFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunidades (e.g., Leiden, Traag et al., 2019) se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(nodos, aristas, covariables) que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generadas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">datos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">persiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">texto pueden superar los l√≠mites de las ventanas de contexto de los LLM. La 'respuesta global' a una consulta dada </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">se produce mediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reportan relevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(e.g., entidades), aristas (e.g., relaciones) y covariables (e.g., afirmaciones) que han sido detectadas, extra√≠das</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">y resumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">c√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag.\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'Graph RAG combina recuperaci√≥n de informaci√≥n y generaci√≥n de res√∫menes enfocados en consultas.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'RAG es ineficaz para preguntas globales sobre un corpus completo.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'El enfoque Graph RAG mejora la exhaustividad y diversidad de respuestas en comparaci√≥n con un RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ingenuo.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'¬øQu√© m√©tricas se utilizar√°n para evaluar la efectividad del enfoque Graph RAG?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'¬øC√≥mo se integrar√°n las fuentes de conocimiento externas en el nuevo sistema?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'¬øQu√© mejoras espec√≠ficas se esperan en comparaci√≥n con los m√©todos anteriores?'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"El\u001b[0m\u001b[32m documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en \u001b[0m\n",
       "\u001b[32mconsultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m al abordar \u001b[0m\n",
       "\u001b[32mpreguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en\u001b[0m\n",
       "\u001b[32mlos textos fuente. Con la aparici√≥n de modelos de lenguaje grandes \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, ya estamos presenciando intentos de \u001b[0m\n",
       "\u001b[32mautomatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el \u001b[0m\n",
       "\u001b[32man√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para \u001b[0m\n",
       "\u001b[32mtareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la \u001b[0m\n",
       "\u001b[32m'resumici√≥n enfocada en consultas' \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS, Dang, 2006\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, particularmente la 'resumici√≥n abstr√°ctica enfocada en \u001b[0m\n",
       "\u001b[32mconsultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de\u001b[0m\n",
       "\u001b[32mQFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la \u001b[0m\n",
       "\u001b[32mrespuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para \u001b[0m\n",
       "\u001b[32mconstruir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de \u001b[0m\n",
       "\u001b[32mentidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de \u001b[0m\n",
       "\u001b[32mcomunidades \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Leiden, Traag et al., 2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos\u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mnodos, aristas, covariables\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de \u001b[0m\n",
       "\u001b[32mconsulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas \u001b[0m\n",
       "\u001b[32mgeneradas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de \u001b[0m\n",
       "\u001b[32mdatos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o \u001b[0m\n",
       "\u001b[32mpersiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de \u001b[0m\n",
       "\u001b[32mtexto pueden superar los l√≠mites de las ventanas de contexto de los LLM. La 'respuesta global' a una consulta dada \u001b[0m\n",
       "\u001b[32mse produce mediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que \u001b[0m\n",
       "\u001b[32mreportan relevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32me.g., entidades\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, aristas \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., relaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m y covariables \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., afirmaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que han sido detectadas, extra√≠das\u001b[0m\n",
       "\u001b[32my resumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de \u001b[0m\n",
       "\u001b[32mc√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag.\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'Graph RAG combina recuperaci√≥n de informaci√≥n y generaci√≥n de res√∫menes enfocados en consultas.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'RAG es ineficaz para preguntas globales sobre un corpus completo.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'El enfoque Graph RAG mejora la exhaustividad y diversidad de respuestas en comparaci√≥n con un RAG \u001b[0m\n",
       "\u001b[32mingenuo.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'¬øQu√© m√©tricas se utilizar√°n para evaluar la efectividad del enfoque Graph RAG?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'¬øC√≥mo se integrar√°n las fuentes de conocimiento externas en el nuevo sistema?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'¬øQu√© mejoras espec√≠ficas se esperan en comparaci√≥n con los m√©todos anteriores?'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered 6 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"El documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation (RAG) al abordar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">los textos fuente. Con la aparici√≥n de modelos de lenguaje grandes (LLMs), ya estamos presenciando intentos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">automatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'resumici√≥n enfocada en consultas' (QFS, Dang, 2006), particularmente la 'resumici√≥n abstr√°ctica enfocada en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">QFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunidades (e.g., Leiden, Traag et al., 2019) se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(nodos, aristas, covariables) que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generadas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">datos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">persiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">texto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">suficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos (Kuratov et al., 2024; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Liu et al., 2023). Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos (e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades), aristas (e.g., relaciones) y covariables (e.g., afirmaciones) que han sido detectadas, extra√≠das y </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">resumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">c√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag.\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'Graph RAG combina recuperaci√≥n de informaci√≥n y generaci√≥n de res√∫menes enfocados en consultas.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'RAG es ineficaz para preguntas globales sobre un corpus completo.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'El enfoque Graph RAG mejora la exhaustividad y diversidad de respuestas en comparaci√≥n con un RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ingenuo.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'¬øQu√© m√©tricas se utilizar√°n para evaluar la efectividad del enfoque Graph RAG?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'¬øC√≥mo se integrar√°n las fuentes de conocimiento externas en el nuevo sistema?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'¬øQu√© mejoras espec√≠ficas se esperan en comparaci√≥n con los m√©todos anteriores?'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"El\u001b[0m\u001b[32m documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en \u001b[0m\n",
       "\u001b[32mconsultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m al abordar \u001b[0m\n",
       "\u001b[32mpreguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en\u001b[0m\n",
       "\u001b[32mlos textos fuente. Con la aparici√≥n de modelos de lenguaje grandes \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, ya estamos presenciando intentos de \u001b[0m\n",
       "\u001b[32mautomatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el \u001b[0m\n",
       "\u001b[32man√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para \u001b[0m\n",
       "\u001b[32mtareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la \u001b[0m\n",
       "\u001b[32m'resumici√≥n enfocada en consultas' \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS, Dang, 2006\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, particularmente la 'resumici√≥n abstr√°ctica enfocada en \u001b[0m\n",
       "\u001b[32mconsultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de\u001b[0m\n",
       "\u001b[32mQFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la \u001b[0m\n",
       "\u001b[32mrespuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para \u001b[0m\n",
       "\u001b[32mconstruir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de \u001b[0m\n",
       "\u001b[32mentidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de \u001b[0m\n",
       "\u001b[32mcomunidades \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Leiden, Traag et al., 2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos\u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mnodos, aristas, covariables\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de \u001b[0m\n",
       "\u001b[32mconsulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas \u001b[0m\n",
       "\u001b[32mgeneradas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de \u001b[0m\n",
       "\u001b[32mdatos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o \u001b[0m\n",
       "\u001b[32mpersiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de \u001b[0m\n",
       "\u001b[32mtexto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar \u001b[0m\n",
       "\u001b[32mampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser \u001b[0m\n",
       "\u001b[32msuficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuratov et al., 2024; \u001b[0m\n",
       "\u001b[32mLiu et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea \u001b[0m\n",
       "\u001b[32minadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque \u001b[0m\n",
       "\u001b[32mRAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce \u001b[0m\n",
       "\u001b[32mmediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan \u001b[0m\n",
       "\u001b[32mrelevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \u001b[0m\n",
       "\u001b[32mentidades\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, aristas \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., relaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m y covariables \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., afirmaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que han sido detectadas, extra√≠das y \u001b[0m\n",
       "\u001b[32mresumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de \u001b[0m\n",
       "\u001b[32mc√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag.\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'Graph RAG combina recuperaci√≥n de informaci√≥n y generaci√≥n de res√∫menes enfocados en consultas.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'RAG es ineficaz para preguntas globales sobre un corpus completo.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'El enfoque Graph RAG mejora la exhaustividad y diversidad de respuestas en comparaci√≥n con un RAG \u001b[0m\n",
       "\u001b[32mingenuo.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'¬øQu√© m√©tricas se utilizar√°n para evaluar la efectividad del enfoque Graph RAG?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'¬øC√≥mo se integrar√°n las fuentes de conocimiento externas en el nuevo sistema?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'¬øQu√© mejoras espec√≠ficas se esperan en comparaci√≥n con los m√©todos anteriores?'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered 7 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"El documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation (RAG) al abordar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">los textos fuente. Con la aparici√≥n de modelos de lenguaje grandes (LLMs), ya estamos presenciando intentos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">automatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'resumici√≥n enfocada en consultas' (QFS, Dang, 2006), particularmente la 'resumici√≥n abstr√°ctica enfocada en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">QFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunidades (e.g., Leiden, Traag et al., 2019) se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(nodos, aristas, covariables) que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generadas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">datos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">persiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">texto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">suficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos (Kuratov et al., 2024; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Liu et al., 2023). Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos (e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades), aristas (e.g., relaciones) y covariables (e.g., afirmaciones) que han sido detectadas, extra√≠das y </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">resumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">c√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento (chunk size) a 600, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documentos de entrada que representan. La resumici√≥n enfocada en consultas de un corpus completo se hace posible </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">utilizando un enfoque de map-reduce: primero usando cada resumen comunitario para responder a la consulta de manera</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">independiente y en paralelo, luego resumiendo todas las respuestas parciales relevantes en una respuesta global </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">final. Para evaluar este enfoque, utilizamos un LLM para generar un conjunto diverso de preguntas centradas en la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">actividad a partir de breves descripciones de dos conjuntos de datos del mundo real representativos, que contienen </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">transcripciones de p√≥dcast y art√≠culos de noticias respectivamente. Para las cualidades objetivo de exhaustividad, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">diversidad y empoderamiento (definidas en la subsecci√≥n 3.4) que desarrollan la comprensi√≥n de problemas y temas </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">amplios, exploramos tanto el impacto de variar el nivel jer√°rquico de los res√∫menes comunitarios.\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'Graph RAG combina recuperaci√≥n de informaci√≥n y generaci√≥n de res√∫menes enfocados en consultas.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'RAG es ineficaz para preguntas globales sobre un corpus completo.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'El enfoque Graph RAG mejora la exhaustividad y diversidad de respuestas en comparaci√≥n con un}'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"El\u001b[0m\u001b[32m documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en \u001b[0m\n",
       "\u001b[32mconsultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m al abordar \u001b[0m\n",
       "\u001b[32mpreguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en\u001b[0m\n",
       "\u001b[32mlos textos fuente. Con la aparici√≥n de modelos de lenguaje grandes \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, ya estamos presenciando intentos de \u001b[0m\n",
       "\u001b[32mautomatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el \u001b[0m\n",
       "\u001b[32man√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para \u001b[0m\n",
       "\u001b[32mtareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la \u001b[0m\n",
       "\u001b[32m'resumici√≥n enfocada en consultas' \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS, Dang, 2006\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, particularmente la 'resumici√≥n abstr√°ctica enfocada en \u001b[0m\n",
       "\u001b[32mconsultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de\u001b[0m\n",
       "\u001b[32mQFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la \u001b[0m\n",
       "\u001b[32mrespuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para \u001b[0m\n",
       "\u001b[32mconstruir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de \u001b[0m\n",
       "\u001b[32mentidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de \u001b[0m\n",
       "\u001b[32mcomunidades \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Leiden, Traag et al., 2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos\u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mnodos, aristas, covariables\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de \u001b[0m\n",
       "\u001b[32mconsulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas \u001b[0m\n",
       "\u001b[32mgeneradas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de \u001b[0m\n",
       "\u001b[32mdatos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o \u001b[0m\n",
       "\u001b[32mpersiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de \u001b[0m\n",
       "\u001b[32mtexto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar \u001b[0m\n",
       "\u001b[32mampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser \u001b[0m\n",
       "\u001b[32msuficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuratov et al., 2024; \u001b[0m\n",
       "\u001b[32mLiu et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea \u001b[0m\n",
       "\u001b[32minadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque \u001b[0m\n",
       "\u001b[32mRAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce \u001b[0m\n",
       "\u001b[32mmediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan \u001b[0m\n",
       "\u001b[32mrelevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \u001b[0m\n",
       "\u001b[32mentidades\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, aristas \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., relaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m y covariables \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., afirmaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que han sido detectadas, extra√≠das y \u001b[0m\n",
       "\u001b[32mresumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de \u001b[0m\n",
       "\u001b[32mc√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 \u001b[0m\n",
       "\u001b[32mextracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento \u001b[0m\u001b[32m(\u001b[0m\u001b[32mchunk size\u001b[0m\u001b[32m)\u001b[0m\u001b[32m a 600, \u001b[0m\n",
       "\u001b[32m1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los\u001b[0m\n",
       "\u001b[32mdocumentos de entrada que representan. La resumici√≥n enfocada en consultas de un corpus completo se hace posible \u001b[0m\n",
       "\u001b[32mutilizando un enfoque de map-reduce: primero usando cada resumen comunitario para responder a la consulta de manera\u001b[0m\n",
       "\u001b[32mindependiente y en paralelo, luego resumiendo todas las respuestas parciales relevantes en una respuesta global \u001b[0m\n",
       "\u001b[32mfinal. Para evaluar este enfoque, utilizamos un LLM para generar un conjunto diverso de preguntas centradas en la \u001b[0m\n",
       "\u001b[32mactividad a partir de breves descripciones de dos conjuntos de datos del mundo real representativos, que contienen \u001b[0m\n",
       "\u001b[32mtranscripciones de p√≥dcast y art√≠culos de noticias respectivamente. Para las cualidades objetivo de exhaustividad, \u001b[0m\n",
       "\u001b[32mdiversidad y empoderamiento \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdefinidas en la subsecci√≥n 3.4\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que desarrollan la comprensi√≥n de problemas y temas \u001b[0m\n",
       "\u001b[32mamplios, exploramos tanto el impacto de variar el nivel jer√°rquico de los res√∫menes comunitarios.\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'Graph RAG combina recuperaci√≥n de informaci√≥n y generaci√≥n de res√∫menes enfocados en consultas.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'RAG es ineficaz para preguntas globales sobre un corpus completo.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'El enfoque Graph RAG mejora la exhaustividad y diversidad de respuestas en comparaci√≥n con un\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered 8 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"El documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation (RAG) al abordar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">los textos fuente. Con la aparici√≥n de modelos de lenguaje grandes (LLMs), ya estamos presenciando intentos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">automatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'resumici√≥n enfocada en consultas' (QFS, Dang, 2006), particularmente la 'resumici√≥n abstr√°ctica enfocada en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">QFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunidades (e.g., Leiden, Traag et al., 2019) se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(nodos, aristas, covariables) que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generadas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">datos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">persiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">texto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">suficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos (Kuratov et al., 2024; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Liu et al., 2023). Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos (e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades), aristas (e.g., relaciones) y covariables (e.g., afirmaciones) que han sido detectadas, extra√≠das y </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">resumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">c√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento (chunk size) a 600, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documentos de entrada que representan. La resumici√≥n enfocada en consultas de un corpus completo se hace posible </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">utilizando un enfoque de map-reduce: primero usando cada resumen comunitario para responder a la consulta de manera</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">independiente y en paralelo, luego resumiendo todas las respuestas parciales relevantes en una respuesta global </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">final. Para evaluar este enfoque, utilizamos un LLM para generar un conjunto diverso de preguntas centradas en la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">actividad a partir de breves descripciones de dos conjuntos de datos del mundo real representativos, que contienen </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">transcripciones de p√≥dcast y art√≠culos de noticias respectivamente. Para las cualidades objetivo de exhaustividad, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">diversidad y empoderamiento (definidas en la subsecci√≥n 3.4) que desarrollan la comprensi√≥n de problemas y temas </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">amplios, exploramos tanto el impacto de variar el nivel jer√°rquico de los res√∫menes comunitarios utilizados para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">responder consultas, como la comparaci√≥n con RAG ingenuo y la resumici√≥n global de textos fuente. Mostramos que </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">todos los enfoques globales superan a RAG ingenuo en exhaustividad y diversidad, y que Graph RAG con res√∫menes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunitarios de nivel intermedio y bajo muestra un rendimiento}\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"El\u001b[0m\u001b[32m documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en \u001b[0m\n",
       "\u001b[32mconsultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m al abordar \u001b[0m\n",
       "\u001b[32mpreguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en\u001b[0m\n",
       "\u001b[32mlos textos fuente. Con la aparici√≥n de modelos de lenguaje grandes \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, ya estamos presenciando intentos de \u001b[0m\n",
       "\u001b[32mautomatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el \u001b[0m\n",
       "\u001b[32man√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para \u001b[0m\n",
       "\u001b[32mtareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la \u001b[0m\n",
       "\u001b[32m'resumici√≥n enfocada en consultas' \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS, Dang, 2006\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, particularmente la 'resumici√≥n abstr√°ctica enfocada en \u001b[0m\n",
       "\u001b[32mconsultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de\u001b[0m\n",
       "\u001b[32mQFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la \u001b[0m\n",
       "\u001b[32mrespuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para \u001b[0m\n",
       "\u001b[32mconstruir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de \u001b[0m\n",
       "\u001b[32mentidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de \u001b[0m\n",
       "\u001b[32mcomunidades \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Leiden, Traag et al., 2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos\u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mnodos, aristas, covariables\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de \u001b[0m\n",
       "\u001b[32mconsulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas \u001b[0m\n",
       "\u001b[32mgeneradas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de \u001b[0m\n",
       "\u001b[32mdatos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o \u001b[0m\n",
       "\u001b[32mpersiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de \u001b[0m\n",
       "\u001b[32mtexto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar \u001b[0m\n",
       "\u001b[32mampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser \u001b[0m\n",
       "\u001b[32msuficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuratov et al., 2024; \u001b[0m\n",
       "\u001b[32mLiu et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea \u001b[0m\n",
       "\u001b[32minadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque \u001b[0m\n",
       "\u001b[32mRAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce \u001b[0m\n",
       "\u001b[32mmediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan \u001b[0m\n",
       "\u001b[32mrelevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \u001b[0m\n",
       "\u001b[32mentidades\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, aristas \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., relaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m y covariables \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., afirmaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que han sido detectadas, extra√≠das y \u001b[0m\n",
       "\u001b[32mresumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de \u001b[0m\n",
       "\u001b[32mc√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 \u001b[0m\n",
       "\u001b[32mextracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento \u001b[0m\u001b[32m(\u001b[0m\u001b[32mchunk size\u001b[0m\u001b[32m)\u001b[0m\u001b[32m a 600, \u001b[0m\n",
       "\u001b[32m1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los\u001b[0m\n",
       "\u001b[32mdocumentos de entrada que representan. La resumici√≥n enfocada en consultas de un corpus completo se hace posible \u001b[0m\n",
       "\u001b[32mutilizando un enfoque de map-reduce: primero usando cada resumen comunitario para responder a la consulta de manera\u001b[0m\n",
       "\u001b[32mindependiente y en paralelo, luego resumiendo todas las respuestas parciales relevantes en una respuesta global \u001b[0m\n",
       "\u001b[32mfinal. Para evaluar este enfoque, utilizamos un LLM para generar un conjunto diverso de preguntas centradas en la \u001b[0m\n",
       "\u001b[32mactividad a partir de breves descripciones de dos conjuntos de datos del mundo real representativos, que contienen \u001b[0m\n",
       "\u001b[32mtranscripciones de p√≥dcast y art√≠culos de noticias respectivamente. Para las cualidades objetivo de exhaustividad, \u001b[0m\n",
       "\u001b[32mdiversidad y empoderamiento \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdefinidas en la subsecci√≥n 3.4\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que desarrollan la comprensi√≥n de problemas y temas \u001b[0m\n",
       "\u001b[32mamplios, exploramos tanto el impacto de variar el nivel jer√°rquico de los res√∫menes comunitarios utilizados para \u001b[0m\n",
       "\u001b[32mresponder consultas, como la comparaci√≥n con RAG ingenuo y la resumici√≥n global de textos fuente. Mostramos que \u001b[0m\n",
       "\u001b[32mtodos los enfoques globales superan a RAG ingenuo en exhaustividad y diversidad, y que Graph RAG con res√∫menes \u001b[0m\n",
       "\u001b[32mcomunitarios de nivel intermedio y bajo muestra un rendimiento\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered 9 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"El documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation (RAG) al abordar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">los textos fuente. Con la aparici√≥n de modelos de lenguaje grandes (LLMs), ya estamos presenciando intentos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">automatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'resumici√≥n enfocada en consultas' (QFS, Dang, 2006), particularmente la 'resumici√≥n abstr√°ctica enfocada en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">QFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunidades (e.g., Leiden, Traag et al., 2019) se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(nodos, aristas, covariables) que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generadas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">datos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">persiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">texto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">suficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos (Kuratov et al., 2024; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Liu et al., 2023). Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos (e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades), aristas (e.g., relaciones) y covariables (e.g., afirmaciones) que han sido detectadas, extra√≠das y </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">resumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">c√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento (chunk size) a 600, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documentos de entrada que representan. Longer text chunks require fewer LLM calls for such extraction, but suffer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">from the recall degradation of longer LLM context windows (Kuratov et al., 2024; Liu et al., 2023). This behavior </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">can be observed in Figure 2 in the case of a single extraction round (i.e., with zero gleanings): on a sample </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dataset (HotPotQA, Yang et al., 2018), using a chunk size of 600 token extracted almost twice as many entity </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">references as when using a chunk size of 2400. While more references are generally better, any extraction process </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">needs to balance recall and precision for the target activity. La resumici√≥n enfocada en consultas de un corpus </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">completo se hace posible utilizando un enfoque de map-reduce: primero usando cada resumen comunitario para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">responder a la consulta de manera independiente y en paralelo, luego resumiendo todas las respuestas parciales </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relevantes en una respuesta global final. Para evaluar este enfoque, utilizamos un LLM para generar un conjunto </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">diverso de preguntas centradas en la actividad a partir de breves descripciones de dos conjuntos de datos del mundo</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">real representativos, que contienen transcripciones de}\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"El\u001b[0m\u001b[32m documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en \u001b[0m\n",
       "\u001b[32mconsultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m al abordar \u001b[0m\n",
       "\u001b[32mpreguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en\u001b[0m\n",
       "\u001b[32mlos textos fuente. Con la aparici√≥n de modelos de lenguaje grandes \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, ya estamos presenciando intentos de \u001b[0m\n",
       "\u001b[32mautomatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el \u001b[0m\n",
       "\u001b[32man√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para \u001b[0m\n",
       "\u001b[32mtareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la \u001b[0m\n",
       "\u001b[32m'resumici√≥n enfocada en consultas' \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS, Dang, 2006\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, particularmente la 'resumici√≥n abstr√°ctica enfocada en \u001b[0m\n",
       "\u001b[32mconsultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de\u001b[0m\n",
       "\u001b[32mQFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la \u001b[0m\n",
       "\u001b[32mrespuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para \u001b[0m\n",
       "\u001b[32mconstruir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de \u001b[0m\n",
       "\u001b[32mentidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de \u001b[0m\n",
       "\u001b[32mcomunidades \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Leiden, Traag et al., 2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos\u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mnodos, aristas, covariables\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de \u001b[0m\n",
       "\u001b[32mconsulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas \u001b[0m\n",
       "\u001b[32mgeneradas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de \u001b[0m\n",
       "\u001b[32mdatos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o \u001b[0m\n",
       "\u001b[32mpersiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de \u001b[0m\n",
       "\u001b[32mtexto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar \u001b[0m\n",
       "\u001b[32mampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser \u001b[0m\n",
       "\u001b[32msuficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuratov et al., 2024; \u001b[0m\n",
       "\u001b[32mLiu et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea \u001b[0m\n",
       "\u001b[32minadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque \u001b[0m\n",
       "\u001b[32mRAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce \u001b[0m\n",
       "\u001b[32mmediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan \u001b[0m\n",
       "\u001b[32mrelevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \u001b[0m\n",
       "\u001b[32mentidades\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, aristas \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., relaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m y covariables \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., afirmaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que han sido detectadas, extra√≠das y \u001b[0m\n",
       "\u001b[32mresumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de \u001b[0m\n",
       "\u001b[32mc√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 \u001b[0m\n",
       "\u001b[32mextracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento \u001b[0m\u001b[32m(\u001b[0m\u001b[32mchunk size\u001b[0m\u001b[32m)\u001b[0m\u001b[32m a 600, \u001b[0m\n",
       "\u001b[32m1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los\u001b[0m\n",
       "\u001b[32mdocumentos de entrada que representan. Longer text chunks require fewer LLM calls for such extraction, but suffer \u001b[0m\n",
       "\u001b[32mfrom the recall degradation of longer LLM context windows \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuratov et al., 2024; Liu et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. This behavior \u001b[0m\n",
       "\u001b[32mcan be observed in Figure 2 in the case of a single extraction round \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e., with zero gleanings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: on a sample \u001b[0m\n",
       "\u001b[32mdataset \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHotPotQA, Yang et al., 2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, using a chunk size of 600 token extracted almost twice as many entity \u001b[0m\n",
       "\u001b[32mreferences as when using a chunk size of 2400. While more references are generally better, any extraction process \u001b[0m\n",
       "\u001b[32mneeds to balance recall and precision for the target activity. La resumici√≥n enfocada en consultas de un corpus \u001b[0m\n",
       "\u001b[32mcompleto se hace posible utilizando un enfoque de map-reduce: primero usando cada resumen comunitario para \u001b[0m\n",
       "\u001b[32mresponder a la consulta de manera independiente y en paralelo, luego resumiendo todas las respuestas parciales \u001b[0m\n",
       "\u001b[32mrelevantes en una respuesta global final. Para evaluar este enfoque, utilizamos un LLM para generar un conjunto \u001b[0m\n",
       "\u001b[32mdiverso de preguntas centradas en la actividad a partir de breves descripciones de dos conjuntos de datos del mundo\u001b[0m\n",
       "\u001b[32mreal representativos, que contienen transcripciones de\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered 10 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"El documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation (RAG) al abordar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">los textos fuente. Con la aparici√≥n de modelos de lenguaje grandes (LLMs), ya estamos presenciando intentos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">automatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'resumici√≥n enfocada en consultas' (QFS, Dang, 2006), particularmente la 'resumici√≥n abstr√°ctica enfocada en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">QFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunidades (e.g., Leiden, Traag et al., 2019) se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(nodos, aristas, covariables) que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generadas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">datos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">persiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">texto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">suficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos (Kuratov et al., 2024; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Liu et al., 2023). Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos (e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades), aristas (e.g., relaciones) y covariables (e.g., afirmaciones) que han sido detectadas, extra√≠das y </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">resumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">c√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento (chunk size) a 600, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documentos de entrada que representan. Los fragmentos de texto m√°s largos requieren menos llamadas al LLM para tal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n, pero sufren de la degradaci√≥n de recuperaci√≥n de ventanas de contexto m√°s largas de LLM (Kuratov et </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">al., 2024; Liu et al., 2023). Este comportamiento puede observarse en la Figura 2 en el caso de una √∫nica ronda de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n (es decir, con cero gleanings): en un conjunto de datos de muestra (HotPotQA, Yang et al., 2018), </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">utilizando un tama√±o de fragmento de 600 tokens se extrajeron casi el doble de referencias de entidades que al </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">utilizar un tama√±o de fragmento de 2400. Si bien m√°s referencias son generalmente mejores, cualquier proceso de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n necesita equilibrar la recuperaci√≥n y la precisi√≥n para la actividad objetivo. La resumici√≥n enfocada en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas de un corpus completo se hace posible utilizando un enfoque de map-reduce: primero usando cada resumen </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunitario para responder a la consulta de manera independiente y en paralelo, luego resumiendo todas las </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respuestas parciales relevantes en una respuesta global final. Para evaluar este enfoque, utilizamos un LLM para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generar un conjunto diverso de preguntas}\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"El\u001b[0m\u001b[32m documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en \u001b[0m\n",
       "\u001b[32mconsultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m al abordar \u001b[0m\n",
       "\u001b[32mpreguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en\u001b[0m\n",
       "\u001b[32mlos textos fuente. Con la aparici√≥n de modelos de lenguaje grandes \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, ya estamos presenciando intentos de \u001b[0m\n",
       "\u001b[32mautomatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el \u001b[0m\n",
       "\u001b[32man√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para \u001b[0m\n",
       "\u001b[32mtareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la \u001b[0m\n",
       "\u001b[32m'resumici√≥n enfocada en consultas' \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS, Dang, 2006\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, particularmente la 'resumici√≥n abstr√°ctica enfocada en \u001b[0m\n",
       "\u001b[32mconsultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de\u001b[0m\n",
       "\u001b[32mQFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la \u001b[0m\n",
       "\u001b[32mrespuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para \u001b[0m\n",
       "\u001b[32mconstruir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de \u001b[0m\n",
       "\u001b[32mentidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de \u001b[0m\n",
       "\u001b[32mcomunidades \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Leiden, Traag et al., 2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos\u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mnodos, aristas, covariables\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de \u001b[0m\n",
       "\u001b[32mconsulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas \u001b[0m\n",
       "\u001b[32mgeneradas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de \u001b[0m\n",
       "\u001b[32mdatos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o \u001b[0m\n",
       "\u001b[32mpersiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de \u001b[0m\n",
       "\u001b[32mtexto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar \u001b[0m\n",
       "\u001b[32mampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser \u001b[0m\n",
       "\u001b[32msuficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuratov et al., 2024; \u001b[0m\n",
       "\u001b[32mLiu et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea \u001b[0m\n",
       "\u001b[32minadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque \u001b[0m\n",
       "\u001b[32mRAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce \u001b[0m\n",
       "\u001b[32mmediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan \u001b[0m\n",
       "\u001b[32mrelevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \u001b[0m\n",
       "\u001b[32mentidades\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, aristas \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., relaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m y covariables \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., afirmaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que han sido detectadas, extra√≠das y \u001b[0m\n",
       "\u001b[32mresumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de \u001b[0m\n",
       "\u001b[32mc√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 \u001b[0m\n",
       "\u001b[32mextracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento \u001b[0m\u001b[32m(\u001b[0m\u001b[32mchunk size\u001b[0m\u001b[32m)\u001b[0m\u001b[32m a 600, \u001b[0m\n",
       "\u001b[32m1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los\u001b[0m\n",
       "\u001b[32mdocumentos de entrada que representan. Los fragmentos de texto m√°s largos requieren menos llamadas al LLM para tal \u001b[0m\n",
       "\u001b[32mextracci√≥n, pero sufren de la degradaci√≥n de recuperaci√≥n de ventanas de contexto m√°s largas de LLM \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuratov et \u001b[0m\n",
       "\u001b[32mal., 2024; Liu et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Este comportamiento puede observarse en la Figura 2 en el caso de una √∫nica ronda de \u001b[0m\n",
       "\u001b[32mextracci√≥n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mes decir, con cero gleanings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: en un conjunto de datos de muestra \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHotPotQA, Yang et al., 2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mutilizando un tama√±o de fragmento de 600 tokens se extrajeron casi el doble de referencias de entidades que al \u001b[0m\n",
       "\u001b[32mutilizar un tama√±o de fragmento de 2400. Si bien m√°s referencias son generalmente mejores, cualquier proceso de \u001b[0m\n",
       "\u001b[32mextracci√≥n necesita equilibrar la recuperaci√≥n y la precisi√≥n para la actividad objetivo. La resumici√≥n enfocada en\u001b[0m\n",
       "\u001b[32mconsultas de un corpus completo se hace posible utilizando un enfoque de map-reduce: primero usando cada resumen \u001b[0m\n",
       "\u001b[32mcomunitario para responder a la consulta de manera independiente y en paralelo, luego resumiendo todas las \u001b[0m\n",
       "\u001b[32mrespuestas parciales relevantes en una respuesta global final. Para evaluar este enfoque, utilizamos un LLM para \u001b[0m\n",
       "\u001b[32mgenerar un conjunto diverso de preguntas\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered 11 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"El documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation (RAG) al abordar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">los textos fuente. Con la aparici√≥n de modelos de lenguaje grandes (LLMs), ya estamos presenciando intentos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">automatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'resumici√≥n enfocada en consultas' (QFS, Dang, 2006), particularmente la 'resumici√≥n abstr√°ctica enfocada en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">QFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunidades (e.g., Leiden, Traag et al., 2019) se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(nodos, aristas, covariables) que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generadas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">datos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">persiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">texto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">suficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos (Kuratov et al., 2024; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Liu et al., 2023). Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos (e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades), aristas (e.g., relaciones) y covariables (e.g., afirmaciones) que han sido detectadas, extra√≠das y </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">resumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">c√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento (chunk size) a 600, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documentos de entrada que representan. Los fragmentos de texto m√°s largos requieren menos llamadas al LLM para tal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n, pero sufren de la degradaci√≥n de recuperaci√≥n de ventanas de contexto m√°s largas de LLM (Kuratov et </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">al., 2024; Liu et al., 2023). Este comportamiento puede observarse en la Figura 2 en el caso de una √∫nica ronda de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n (es decir, con cero gleanings): en un conjunto de datos de muestra (HotPotQA, Yang et al., 2018), </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">utilizando un tama√±o de fragmento de 600 tokens se extrajeron casi el doble de referencias de entidades que al </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">utilizar un tama√±o de fragmento de 2400. Si bien m√°s referencias son generalmente mejores, cualquier proceso de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n necesita equilibrar la recuperaci√≥n y la precisi√≥n para la actividad objetivo. La resumici√≥n enfocada en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas de un corpus completo se hace posible utilizando un enfoque de map-reduce: primero usando cada resumen </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunitario para responder a la consulta de manera independiente y en paralelo, luego resumiendo todas las </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respuestas parciales relevantes en una respuesta global final. Para evaluar este enfoque, utilizamos un LLM para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generar un conjunto diverso de preguntas}\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"El\u001b[0m\u001b[32m documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en \u001b[0m\n",
       "\u001b[32mconsultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m al abordar \u001b[0m\n",
       "\u001b[32mpreguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en\u001b[0m\n",
       "\u001b[32mlos textos fuente. Con la aparici√≥n de modelos de lenguaje grandes \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, ya estamos presenciando intentos de \u001b[0m\n",
       "\u001b[32mautomatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el \u001b[0m\n",
       "\u001b[32man√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para \u001b[0m\n",
       "\u001b[32mtareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la \u001b[0m\n",
       "\u001b[32m'resumici√≥n enfocada en consultas' \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS, Dang, 2006\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, particularmente la 'resumici√≥n abstr√°ctica enfocada en \u001b[0m\n",
       "\u001b[32mconsultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de\u001b[0m\n",
       "\u001b[32mQFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la \u001b[0m\n",
       "\u001b[32mrespuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para \u001b[0m\n",
       "\u001b[32mconstruir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de \u001b[0m\n",
       "\u001b[32mentidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de \u001b[0m\n",
       "\u001b[32mcomunidades \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Leiden, Traag et al., 2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos\u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mnodos, aristas, covariables\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de \u001b[0m\n",
       "\u001b[32mconsulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas \u001b[0m\n",
       "\u001b[32mgeneradas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de \u001b[0m\n",
       "\u001b[32mdatos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o \u001b[0m\n",
       "\u001b[32mpersiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de \u001b[0m\n",
       "\u001b[32mtexto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar \u001b[0m\n",
       "\u001b[32mampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser \u001b[0m\n",
       "\u001b[32msuficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuratov et al., 2024; \u001b[0m\n",
       "\u001b[32mLiu et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea \u001b[0m\n",
       "\u001b[32minadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque \u001b[0m\n",
       "\u001b[32mRAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce \u001b[0m\n",
       "\u001b[32mmediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan \u001b[0m\n",
       "\u001b[32mrelevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \u001b[0m\n",
       "\u001b[32mentidades\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, aristas \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., relaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m y covariables \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., afirmaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que han sido detectadas, extra√≠das y \u001b[0m\n",
       "\u001b[32mresumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de \u001b[0m\n",
       "\u001b[32mc√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 \u001b[0m\n",
       "\u001b[32mextracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento \u001b[0m\u001b[32m(\u001b[0m\u001b[32mchunk size\u001b[0m\u001b[32m)\u001b[0m\u001b[32m a 600, \u001b[0m\n",
       "\u001b[32m1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los\u001b[0m\n",
       "\u001b[32mdocumentos de entrada que representan. Los fragmentos de texto m√°s largos requieren menos llamadas al LLM para tal \u001b[0m\n",
       "\u001b[32mextracci√≥n, pero sufren de la degradaci√≥n de recuperaci√≥n de ventanas de contexto m√°s largas de LLM \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuratov et \u001b[0m\n",
       "\u001b[32mal., 2024; Liu et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Este comportamiento puede observarse en la Figura 2 en el caso de una √∫nica ronda de \u001b[0m\n",
       "\u001b[32mextracci√≥n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mes decir, con cero gleanings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: en un conjunto de datos de muestra \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHotPotQA, Yang et al., 2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mutilizando un tama√±o de fragmento de 600 tokens se extrajeron casi el doble de referencias de entidades que al \u001b[0m\n",
       "\u001b[32mutilizar un tama√±o de fragmento de 2400. Si bien m√°s referencias son generalmente mejores, cualquier proceso de \u001b[0m\n",
       "\u001b[32mextracci√≥n necesita equilibrar la recuperaci√≥n y la precisi√≥n para la actividad objetivo. La resumici√≥n enfocada en\u001b[0m\n",
       "\u001b[32mconsultas de un corpus completo se hace posible utilizando un enfoque de map-reduce: primero usando cada resumen \u001b[0m\n",
       "\u001b[32mcomunitario para responder a la consulta de manera independiente y en paralelo, luego resumiendo todas las \u001b[0m\n",
       "\u001b[32mrespuestas parciales relevantes en una respuesta global final. Para evaluar este enfoque, utilizamos un LLM para \u001b[0m\n",
       "\u001b[32mgenerar un conjunto diverso de preguntas\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered 12 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"El documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation (RAG) al abordar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">los textos fuente. Con la aparici√≥n de modelos de lenguaje grandes (LLMs), ya estamos presenciando intentos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">automatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'resumici√≥n enfocada en consultas' (QFS, Dang, 2006), particularmente la 'resumici√≥n abstr√°ctica enfocada en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">QFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunidades (e.g., Leiden, Traag et al., 2019) se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(nodos, aristas, covariables) que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generadas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">datos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">persiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">texto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">suficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos (Kuratov et al., 2024; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Liu et al., 2023). Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos (e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades), aristas (e.g., relaciones) y covariables (e.g., afirmaciones) que han sido detectadas, extra√≠das y </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">resumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">c√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento (chunk size) a 600, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documentos de entrada que representan. Los fragmentos de texto m√°s largos requieren menos llamadas al LLM para tal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n, pero sufren de la degradaci√≥n de recuperaci√≥n de ventanas de contexto m√°s largas de LLM (Kuratov et </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">al., 2024; Liu et al., 2023). Este comportamiento puede observarse en la Figura 2 en el caso de una √∫nica ronda de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n (es decir, con cero gleanings): en un conjunto de datos de muestra (HotPotQA, Yang et al., 2018), </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">utilizando un tama√±o de fragmento de 600 tokens se extrajeron casi el doble de referencias de entidades que al </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">utilizar un tama√±o de fragmento de 2400. Si bien m√°s referencias son generalmente mejores, cualquier proceso de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n necesita equilibrar la recuperaci√≥n y la precisi√≥n para la actividad objetivo. La resumici√≥n enfocada en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas de un corpus completo se hace posible utilizando un enfoque de map-reduce: primero usando cada resumen </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunitario para responder a la consulta de manera independiente y en paralelo, luego resumiendo todas las </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respuestas parciales relevantes en una respuesta global final. Para evaluar este enfoque, utilizamos un LLM para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generar un conjunto diverso de preguntas. Sin embargo, existe una}\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"El\u001b[0m\u001b[32m documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en \u001b[0m\n",
       "\u001b[32mconsultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m al abordar \u001b[0m\n",
       "\u001b[32mpreguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en\u001b[0m\n",
       "\u001b[32mlos textos fuente. Con la aparici√≥n de modelos de lenguaje grandes \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, ya estamos presenciando intentos de \u001b[0m\n",
       "\u001b[32mautomatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el \u001b[0m\n",
       "\u001b[32man√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para \u001b[0m\n",
       "\u001b[32mtareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la \u001b[0m\n",
       "\u001b[32m'resumici√≥n enfocada en consultas' \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS, Dang, 2006\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, particularmente la 'resumici√≥n abstr√°ctica enfocada en \u001b[0m\n",
       "\u001b[32mconsultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de\u001b[0m\n",
       "\u001b[32mQFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la \u001b[0m\n",
       "\u001b[32mrespuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para \u001b[0m\n",
       "\u001b[32mconstruir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de \u001b[0m\n",
       "\u001b[32mentidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de \u001b[0m\n",
       "\u001b[32mcomunidades \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Leiden, Traag et al., 2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos\u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mnodos, aristas, covariables\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de \u001b[0m\n",
       "\u001b[32mconsulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas \u001b[0m\n",
       "\u001b[32mgeneradas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de \u001b[0m\n",
       "\u001b[32mdatos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o \u001b[0m\n",
       "\u001b[32mpersiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de \u001b[0m\n",
       "\u001b[32mtexto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar \u001b[0m\n",
       "\u001b[32mampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser \u001b[0m\n",
       "\u001b[32msuficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuratov et al., 2024; \u001b[0m\n",
       "\u001b[32mLiu et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea \u001b[0m\n",
       "\u001b[32minadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque \u001b[0m\n",
       "\u001b[32mRAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce \u001b[0m\n",
       "\u001b[32mmediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan \u001b[0m\n",
       "\u001b[32mrelevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \u001b[0m\n",
       "\u001b[32mentidades\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, aristas \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., relaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m y covariables \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., afirmaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que han sido detectadas, extra√≠das y \u001b[0m\n",
       "\u001b[32mresumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de \u001b[0m\n",
       "\u001b[32mc√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 \u001b[0m\n",
       "\u001b[32mextracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento \u001b[0m\u001b[32m(\u001b[0m\u001b[32mchunk size\u001b[0m\u001b[32m)\u001b[0m\u001b[32m a 600, \u001b[0m\n",
       "\u001b[32m1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los\u001b[0m\n",
       "\u001b[32mdocumentos de entrada que representan. Los fragmentos de texto m√°s largos requieren menos llamadas al LLM para tal \u001b[0m\n",
       "\u001b[32mextracci√≥n, pero sufren de la degradaci√≥n de recuperaci√≥n de ventanas de contexto m√°s largas de LLM \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuratov et \u001b[0m\n",
       "\u001b[32mal., 2024; Liu et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Este comportamiento puede observarse en la Figura 2 en el caso de una √∫nica ronda de \u001b[0m\n",
       "\u001b[32mextracci√≥n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mes decir, con cero gleanings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: en un conjunto de datos de muestra \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHotPotQA, Yang et al., 2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mutilizando un tama√±o de fragmento de 600 tokens se extrajeron casi el doble de referencias de entidades que al \u001b[0m\n",
       "\u001b[32mutilizar un tama√±o de fragmento de 2400. Si bien m√°s referencias son generalmente mejores, cualquier proceso de \u001b[0m\n",
       "\u001b[32mextracci√≥n necesita equilibrar la recuperaci√≥n y la precisi√≥n para la actividad objetivo. La resumici√≥n enfocada en\u001b[0m\n",
       "\u001b[32mconsultas de un corpus completo se hace posible utilizando un enfoque de map-reduce: primero usando cada resumen \u001b[0m\n",
       "\u001b[32mcomunitario para responder a la consulta de manera independiente y en paralelo, luego resumiendo todas las \u001b[0m\n",
       "\u001b[32mrespuestas parciales relevantes en una respuesta global final. Para evaluar este enfoque, utilizamos un LLM para \u001b[0m\n",
       "\u001b[32mgenerar un conjunto diverso de preguntas. Sin embargo, existe una\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered 13 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"El documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation (RAG) al abordar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">los textos fuente. Con la aparici√≥n de modelos de lenguaje grandes (LLMs), ya estamos presenciando intentos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">automatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'resumici√≥n enfocada en consultas' (QFS, Dang, 2006), particularmente la 'resumici√≥n abstr√°ctica enfocada en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">QFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunidades (e.g., Leiden, Traag et al., 2019) se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(nodos, aristas, covariables) que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generadas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">datos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">persiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">texto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">suficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos (Kuratov et al., 2024; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Liu et al., 2023). Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos (e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades), aristas (e.g., relaciones) y covariables (e.g., afirmaciones) que han sido detectadas, extra√≠das y </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">resumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">c√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento (chunk size) a 600, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documentos de entrada que representan. Los fragmentos de texto m√°s largos requieren menos llamadas al LLM para tal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n, pero sufren de la degradaci√≥n de recuperaci√≥n de ventanas de contexto m√°s largas de LLM (Kuratov et </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">al., 2024; Liu et al., 2023). Este comportamiento puede observarse en la Figura 2 en el caso de una √∫nica ronda de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n (es decir, con cero gleanings): en un conjunto de datos de muestra (HotPotQA, Yang et al., 2018), </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">utilizando un tama√±o de fragmento de 600 tokens se extrajeron casi el doble de referencias de entidades que al </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">utilizar un tama√±o de fragmento de 2400. Si bien m√°s referencias son generalmente mejores, cualquier proceso de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n necesita equilibrar la recuperaci√≥n y la precisi√≥n para la actividad objetivo. La resumici√≥n enfocada en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas de un corpus completo se hace posible utilizando un enfoque de map-reduce: primero usando cada resumen </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunitario para responder a la consulta de manera independiente y en paralelo, luego resumiendo todas las </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respuestas parciales relevantes en una respuesta global final. Para evaluar este enfoque, utilizamos un LLM para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generar un conjunto diverso de preguntas}\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"El\u001b[0m\u001b[32m documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en \u001b[0m\n",
       "\u001b[32mconsultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m al abordar \u001b[0m\n",
       "\u001b[32mpreguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en\u001b[0m\n",
       "\u001b[32mlos textos fuente. Con la aparici√≥n de modelos de lenguaje grandes \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, ya estamos presenciando intentos de \u001b[0m\n",
       "\u001b[32mautomatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el \u001b[0m\n",
       "\u001b[32man√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para \u001b[0m\n",
       "\u001b[32mtareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la \u001b[0m\n",
       "\u001b[32m'resumici√≥n enfocada en consultas' \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS, Dang, 2006\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, particularmente la 'resumici√≥n abstr√°ctica enfocada en \u001b[0m\n",
       "\u001b[32mconsultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de\u001b[0m\n",
       "\u001b[32mQFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la \u001b[0m\n",
       "\u001b[32mrespuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para \u001b[0m\n",
       "\u001b[32mconstruir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de \u001b[0m\n",
       "\u001b[32mentidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de \u001b[0m\n",
       "\u001b[32mcomunidades \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Leiden, Traag et al., 2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos\u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mnodos, aristas, covariables\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de \u001b[0m\n",
       "\u001b[32mconsulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas \u001b[0m\n",
       "\u001b[32mgeneradas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de \u001b[0m\n",
       "\u001b[32mdatos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o \u001b[0m\n",
       "\u001b[32mpersiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de \u001b[0m\n",
       "\u001b[32mtexto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar \u001b[0m\n",
       "\u001b[32mampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser \u001b[0m\n",
       "\u001b[32msuficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuratov et al., 2024; \u001b[0m\n",
       "\u001b[32mLiu et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea \u001b[0m\n",
       "\u001b[32minadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque \u001b[0m\n",
       "\u001b[32mRAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce \u001b[0m\n",
       "\u001b[32mmediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan \u001b[0m\n",
       "\u001b[32mrelevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \u001b[0m\n",
       "\u001b[32mentidades\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, aristas \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., relaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m y covariables \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., afirmaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que han sido detectadas, extra√≠das y \u001b[0m\n",
       "\u001b[32mresumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de \u001b[0m\n",
       "\u001b[32mc√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 \u001b[0m\n",
       "\u001b[32mextracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento \u001b[0m\u001b[32m(\u001b[0m\u001b[32mchunk size\u001b[0m\u001b[32m)\u001b[0m\u001b[32m a 600, \u001b[0m\n",
       "\u001b[32m1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los\u001b[0m\n",
       "\u001b[32mdocumentos de entrada que representan. Los fragmentos de texto m√°s largos requieren menos llamadas al LLM para tal \u001b[0m\n",
       "\u001b[32mextracci√≥n, pero sufren de la degradaci√≥n de recuperaci√≥n de ventanas de contexto m√°s largas de LLM \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuratov et \u001b[0m\n",
       "\u001b[32mal., 2024; Liu et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Este comportamiento puede observarse en la Figura 2 en el caso de una √∫nica ronda de \u001b[0m\n",
       "\u001b[32mextracci√≥n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mes decir, con cero gleanings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: en un conjunto de datos de muestra \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHotPotQA, Yang et al., 2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mutilizando un tama√±o de fragmento de 600 tokens se extrajeron casi el doble de referencias de entidades que al \u001b[0m\n",
       "\u001b[32mutilizar un tama√±o de fragmento de 2400. Si bien m√°s referencias son generalmente mejores, cualquier proceso de \u001b[0m\n",
       "\u001b[32mextracci√≥n necesita equilibrar la recuperaci√≥n y la precisi√≥n para la actividad objetivo. La resumici√≥n enfocada en\u001b[0m\n",
       "\u001b[32mconsultas de un corpus completo se hace posible utilizando un enfoque de map-reduce: primero usando cada resumen \u001b[0m\n",
       "\u001b[32mcomunitario para responder a la consulta de manera independiente y en paralelo, luego resumiendo todas las \u001b[0m\n",
       "\u001b[32mrespuestas parciales relevantes en una respuesta global final. Para evaluar este enfoque, utilizamos un LLM para \u001b[0m\n",
       "\u001b[32mgenerar un conjunto diverso de preguntas\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered 14 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"El documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation (RAG) al abordar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">los textos fuente. Con la aparici√≥n de modelos de lenguaje grandes (LLMs), ya estamos presenciando intentos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">automatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'resumici√≥n enfocada en consultas' (QFS, Dang, 2006), particularmente la 'resumici√≥n abstr√°ctica enfocada en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">QFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunidades (e.g., Leiden, Traag et al., 2019) se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(nodos, aristas, covariables) que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generadas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">datos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">persiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">texto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">suficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos (Kuratov et al., 2024; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Liu et al., 2023). Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos (e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades), aristas (e.g., relaciones) y covariables (e.g., afirmaciones) que han sido detectadas, extra√≠das y </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">resumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">c√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento (chunk size) a 600, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documentos de entrada que representan. Los fragmentos de texto m√°s largos requieren menos llamadas al LLM para tal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n, pero sufren de la degradaci√≥n de recuperaci√≥n de ventanas de contexto m√°s largas de LLM (Kuratov et </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">al., 2024; Liu et al., 2023). Este comportamiento puede observarse en la Figura 2 en el caso de una √∫nica ronda de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n (es decir, con cero gleanings): en un conjunto de datos de muestra (HotPotQA, Yang et al., 2018), </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">utilizando un tama√±o de fragmento de 600 tokens se extrajeron casi el doble de referencias de entidades que al </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">utilizar un tama√±o de fragmento de 2400. Si bien m√°s referencias son generalmente mejores, cualquier proceso de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n necesita equilibrar la recuperaci√≥n y la precisi√≥n para la actividad objetivo. La resumici√≥n enfocada en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas de un corpus completo se hace posible utilizando un enfoque de map-reduce: primero usando cada resumen </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunitario para responder a la consulta de manera independiente y en paralelo, luego resumiendo todas las </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respuestas parciales relevantes en una respuesta global final. Para evaluar este enfoque, utilizamos un LLM para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generar un conjunto diverso de preguntas}\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"El\u001b[0m\u001b[32m documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en \u001b[0m\n",
       "\u001b[32mconsultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m al abordar \u001b[0m\n",
       "\u001b[32mpreguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en\u001b[0m\n",
       "\u001b[32mlos textos fuente. Con la aparici√≥n de modelos de lenguaje grandes \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, ya estamos presenciando intentos de \u001b[0m\n",
       "\u001b[32mautomatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el \u001b[0m\n",
       "\u001b[32man√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para \u001b[0m\n",
       "\u001b[32mtareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la \u001b[0m\n",
       "\u001b[32m'resumici√≥n enfocada en consultas' \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS, Dang, 2006\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, particularmente la 'resumici√≥n abstr√°ctica enfocada en \u001b[0m\n",
       "\u001b[32mconsultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de\u001b[0m\n",
       "\u001b[32mQFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la \u001b[0m\n",
       "\u001b[32mrespuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para \u001b[0m\n",
       "\u001b[32mconstruir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de \u001b[0m\n",
       "\u001b[32mentidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de \u001b[0m\n",
       "\u001b[32mcomunidades \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Leiden, Traag et al., 2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos\u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mnodos, aristas, covariables\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de \u001b[0m\n",
       "\u001b[32mconsulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas \u001b[0m\n",
       "\u001b[32mgeneradas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de \u001b[0m\n",
       "\u001b[32mdatos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o \u001b[0m\n",
       "\u001b[32mpersiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de \u001b[0m\n",
       "\u001b[32mtexto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar \u001b[0m\n",
       "\u001b[32mampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser \u001b[0m\n",
       "\u001b[32msuficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuratov et al., 2024; \u001b[0m\n",
       "\u001b[32mLiu et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea \u001b[0m\n",
       "\u001b[32minadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque \u001b[0m\n",
       "\u001b[32mRAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce \u001b[0m\n",
       "\u001b[32mmediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan \u001b[0m\n",
       "\u001b[32mrelevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \u001b[0m\n",
       "\u001b[32mentidades\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, aristas \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., relaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m y covariables \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., afirmaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que han sido detectadas, extra√≠das y \u001b[0m\n",
       "\u001b[32mresumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de \u001b[0m\n",
       "\u001b[32mc√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 \u001b[0m\n",
       "\u001b[32mextracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento \u001b[0m\u001b[32m(\u001b[0m\u001b[32mchunk size\u001b[0m\u001b[32m)\u001b[0m\u001b[32m a 600, \u001b[0m\n",
       "\u001b[32m1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los\u001b[0m\n",
       "\u001b[32mdocumentos de entrada que representan. Los fragmentos de texto m√°s largos requieren menos llamadas al LLM para tal \u001b[0m\n",
       "\u001b[32mextracci√≥n, pero sufren de la degradaci√≥n de recuperaci√≥n de ventanas de contexto m√°s largas de LLM \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuratov et \u001b[0m\n",
       "\u001b[32mal., 2024; Liu et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Este comportamiento puede observarse en la Figura 2 en el caso de una √∫nica ronda de \u001b[0m\n",
       "\u001b[32mextracci√≥n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mes decir, con cero gleanings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: en un conjunto de datos de muestra \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHotPotQA, Yang et al., 2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mutilizando un tama√±o de fragmento de 600 tokens se extrajeron casi el doble de referencias de entidades que al \u001b[0m\n",
       "\u001b[32mutilizar un tama√±o de fragmento de 2400. Si bien m√°s referencias son generalmente mejores, cualquier proceso de \u001b[0m\n",
       "\u001b[32mextracci√≥n necesita equilibrar la recuperaci√≥n y la precisi√≥n para la actividad objetivo. La resumici√≥n enfocada en\u001b[0m\n",
       "\u001b[32mconsultas de un corpus completo se hace posible utilizando un enfoque de map-reduce: primero usando cada resumen \u001b[0m\n",
       "\u001b[32mcomunitario para responder a la consulta de manera independiente y en paralelo, luego resumiendo todas las \u001b[0m\n",
       "\u001b[32mrespuestas parciales relevantes en una respuesta global final. Para evaluar este enfoque, utilizamos un LLM para \u001b[0m\n",
       "\u001b[32mgenerar un conjunto diverso de preguntas\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered 15 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"El documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation (RAG) al abordar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">los textos fuente. Con la aparici√≥n de modelos de lenguaje grandes (LLMs), ya estamos presenciando intentos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">automatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'resumici√≥n enfocada en consultas' (QFS, Dang, 2006), particularmente la 'resumici√≥n abstr√°ctica enfocada en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">QFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunidades (e.g., Leiden, Traag et al., 2019) se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(nodos, aristas, covariables) que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generadas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">datos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">persiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">texto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">suficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos (Kuratov et al., 2024; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Liu et al., 2023). Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos (e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades), aristas (e.g., relaciones) y covariables (e.g., afirmaciones) que han sido detectadas, extra√≠das y </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">resumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">c√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento (chunk size) a 600, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documentos de entrada que representan. Los fragmentos de texto m√°s largos requieren menos llamadas al LLM para tal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n, pero sufren de la degradaci√≥n de recuperaci√≥n de ventanas de contexto m√°s largas de LLM (Kuratov et </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">al., 2024; Liu et al., 2023). Este comportamiento puede observarse en la Figura 2 en el caso de una √∫nica ronda de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n (es decir, con cero gleanings): en un conjunto de datos de muestra (HotPotQA, Yang et al., 2018), </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">utilizando un tama√±o de fragmento de 600 tokens se extrajeron casi el doble de referencias de entidades que al </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">utilizar un tama√±o de fragmento de 2400. Si bien m√°s referencias son generalmente mejores, cualquier proceso de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n necesita equilibrar la recuperaci√≥n y la precisi√≥n para la actividad objetivo. La resumici√≥n enfocada en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas de un corpus completo se hace posible utilizando un enfoque de map-reduce: primero usando cada resumen </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunitario para responder a la consulta de manera independiente y en paralelo, luego resumiendo todas las </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respuestas parciales relevantes en una respuesta global final. Para evaluar este enfoque, utilizamos un LLM para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generar un conjunto diverso de preguntas}\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"El\u001b[0m\u001b[32m documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en \u001b[0m\n",
       "\u001b[32mconsultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m al abordar \u001b[0m\n",
       "\u001b[32mpreguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en\u001b[0m\n",
       "\u001b[32mlos textos fuente. Con la aparici√≥n de modelos de lenguaje grandes \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, ya estamos presenciando intentos de \u001b[0m\n",
       "\u001b[32mautomatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el \u001b[0m\n",
       "\u001b[32man√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para \u001b[0m\n",
       "\u001b[32mtareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la \u001b[0m\n",
       "\u001b[32m'resumici√≥n enfocada en consultas' \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS, Dang, 2006\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, particularmente la 'resumici√≥n abstr√°ctica enfocada en \u001b[0m\n",
       "\u001b[32mconsultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de\u001b[0m\n",
       "\u001b[32mQFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la \u001b[0m\n",
       "\u001b[32mrespuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para \u001b[0m\n",
       "\u001b[32mconstruir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de \u001b[0m\n",
       "\u001b[32mentidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de \u001b[0m\n",
       "\u001b[32mcomunidades \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Leiden, Traag et al., 2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos\u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mnodos, aristas, covariables\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de \u001b[0m\n",
       "\u001b[32mconsulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas \u001b[0m\n",
       "\u001b[32mgeneradas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de \u001b[0m\n",
       "\u001b[32mdatos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o \u001b[0m\n",
       "\u001b[32mpersiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de \u001b[0m\n",
       "\u001b[32mtexto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar \u001b[0m\n",
       "\u001b[32mampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser \u001b[0m\n",
       "\u001b[32msuficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuratov et al., 2024; \u001b[0m\n",
       "\u001b[32mLiu et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea \u001b[0m\n",
       "\u001b[32minadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque \u001b[0m\n",
       "\u001b[32mRAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce \u001b[0m\n",
       "\u001b[32mmediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan \u001b[0m\n",
       "\u001b[32mrelevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \u001b[0m\n",
       "\u001b[32mentidades\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, aristas \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., relaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m y covariables \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., afirmaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que han sido detectadas, extra√≠das y \u001b[0m\n",
       "\u001b[32mresumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de \u001b[0m\n",
       "\u001b[32mc√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 \u001b[0m\n",
       "\u001b[32mextracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento \u001b[0m\u001b[32m(\u001b[0m\u001b[32mchunk size\u001b[0m\u001b[32m)\u001b[0m\u001b[32m a 600, \u001b[0m\n",
       "\u001b[32m1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los\u001b[0m\n",
       "\u001b[32mdocumentos de entrada que representan. Los fragmentos de texto m√°s largos requieren menos llamadas al LLM para tal \u001b[0m\n",
       "\u001b[32mextracci√≥n, pero sufren de la degradaci√≥n de recuperaci√≥n de ventanas de contexto m√°s largas de LLM \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuratov et \u001b[0m\n",
       "\u001b[32mal., 2024; Liu et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Este comportamiento puede observarse en la Figura 2 en el caso de una √∫nica ronda de \u001b[0m\n",
       "\u001b[32mextracci√≥n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mes decir, con cero gleanings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: en un conjunto de datos de muestra \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHotPotQA, Yang et al., 2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mutilizando un tama√±o de fragmento de 600 tokens se extrajeron casi el doble de referencias de entidades que al \u001b[0m\n",
       "\u001b[32mutilizar un tama√±o de fragmento de 2400. Si bien m√°s referencias son generalmente mejores, cualquier proceso de \u001b[0m\n",
       "\u001b[32mextracci√≥n necesita equilibrar la recuperaci√≥n y la precisi√≥n para la actividad objetivo. La resumici√≥n enfocada en\u001b[0m\n",
       "\u001b[32mconsultas de un corpus completo se hace posible utilizando un enfoque de map-reduce: primero usando cada resumen \u001b[0m\n",
       "\u001b[32mcomunitario para responder a la consulta de manera independiente y en paralelo, luego resumiendo todas las \u001b[0m\n",
       "\u001b[32mrespuestas parciales relevantes en una respuesta global final. Para evaluar este enfoque, utilizamos un LLM para \u001b[0m\n",
       "\u001b[32mgenerar un conjunto diverso de preguntas\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "latest_summary = \"\"\n",
    "\n",
    "## TODO: Use the techniques from the previous notebook to complete the exercise\n",
    "def RSummarizer(knowledge, llm, prompt, verbose=False):\n",
    "    '''\n",
    "    Exercise: Create a chain that summarizes\n",
    "    '''\n",
    "    ###########################################################################################\n",
    "    ## START TODO:\n",
    "\n",
    "    def summarize_docs(docs):\n",
    "        ## TODO: Initialize the parse_chain appropriately; should include an RExtract instance.\n",
    "        parse_chain = (RunnableAssign({'info_base': RExtract(\n",
    "            DocumentSummaryBase, llm, summary_prompt)}))\n",
    "\n",
    "        # Initialize a valid starting state. Should be similar to notebook 4\n",
    "        state = {'info_base': DocumentSummaryBase()}\n",
    "\n",
    "        global latest_summary  ## If your loop crashes, you can check out the latest_summary\n",
    "\n",
    "        for i, doc in enumerate(docs):\n",
    "            ## TODO: Update the state as appropriate using your parse_chain component\n",
    "            state['input'] = doc.page_content\n",
    "            state = parse_chain.invoke(state)\n",
    "\n",
    "            assert 'info_base' in state\n",
    "            if verbose:\n",
    "                print(f\"Considered {i+1} documents\")\n",
    "                pprint(state['info_base'])\n",
    "                latest_summary = state['info_base']\n",
    "                # clear_output(wait=True)\n",
    "\n",
    "        return state['info_base']\n",
    "\n",
    "    ## END TODO\n",
    "    ###########################################################################################\n",
    "\n",
    "    return RunnableLambda(summarize_docs)\n",
    "\n",
    "instruct_model = ChatOpenAI(model=\"gpt-4o-mini\").bind(max_tokens=1024)\n",
    "instruct_llm = instruct_model | StrOutputParser()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "documents = ArxivLoader(query=\"2404.16130\").load()  # GraphRAG\n",
    "\n",
    "# Some nice custom preprocessing\n",
    "documents[0].page_content = documents[0].page_content.replace(\". .\", \"\")\n",
    "docs_split = text_splitter.split_documents(documents)\n",
    "\n",
    "# Take the first 10 document chunks and accumulate a DocumentSummaryBase\n",
    "summarizer = RSummarizer(DocumentSummaryBase(\n",
    "), instruct_llm, summary_prompt, verbose=True)\n",
    "\n",
    "summary = summarizer.invoke(docs_split[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07eb5710-23f7-4782-84eb-1fc8f73500b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"El documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation (RAG) al abordar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">los textos fuente. Con la aparici√≥n de modelos de lenguaje grandes (LLMs), ya estamos presenciando intentos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">automatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'resumici√≥n enfocada en consultas' (QFS, Dang, 2006), particularmente la 'resumici√≥n abstr√°ctica enfocada en </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">QFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunidades (e.g., Leiden, Traag et al., 2019) se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(nodos, aristas, covariables) que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generadas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">datos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">persiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">texto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">suficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos (Kuratov et al., 2024; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Liu et al., 2023). Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos (e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entidades), aristas (e.g., relaciones) y covariables (e.g., afirmaciones) que han sido detectadas, extra√≠das y </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">resumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">c√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento (chunk size) a 600, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documentos de entrada que representan. Los fragmentos de texto m√°s largos requieren menos llamadas al LLM para tal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n, pero sufren de la degradaci√≥n de recuperaci√≥n de ventanas de contexto m√°s largas de LLM (Kuratov et </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">al., 2024; Liu et al., 2023). Este comportamiento puede observarse en la Figura 2 en el caso de una √∫nica ronda de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n (es decir, con cero gleanings): en un conjunto de datos de muestra (HotPotQA, Yang et al., 2018), </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">utilizando un tama√±o de fragmento de 600 tokens se extrajeron casi el doble de referencias de entidades que al </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">utilizar un tama√±o de fragmento de 2400. Si bien m√°s referencias son generalmente mejores, cualquier proceso de </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracci√≥n necesita equilibrar la recuperaci√≥n y la precisi√≥n para la actividad objetivo. La resumici√≥n enfocada en</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consultas de un corpus completo se hace posible utilizando un enfoque de map-reduce: primero usando cada resumen </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comunitario para responder a la consulta de manera independiente y en paralelo, luego resumiendo todas las </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">respuestas parciales relevantes en una respuesta global final. Para evaluar este enfoque, utilizamos un LLM para </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generar un conjunto diverso de preguntas}\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"El\u001b[0m\u001b[32m documento presenta un enfoque Graph RAG para la generaci√≥n de res√∫menes enfocados en \u001b[0m\n",
       "\u001b[32mconsultas, superando las limitaciones de los modelos de recuperaci√≥n-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m al abordar \u001b[0m\n",
       "\u001b[32mpreguntas globales sobre colecciones de textos, a menudo alcanzando conclusiones que van m√°s all√° de lo indicado en\u001b[0m\n",
       "\u001b[32mlos textos fuente. Con la aparici√≥n de modelos de lenguaje grandes \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, ya estamos presenciando intentos de \u001b[0m\n",
       "\u001b[32mautomatizar la comprensi√≥n similar a la humana en dominios complejos como el descubrimiento cient√≠fico y el \u001b[0m\n",
       "\u001b[32man√°lisis de inteligencia. Se destaca que el RAG es efectivo para recuperar informaci√≥n relevante, pero no para \u001b[0m\n",
       "\u001b[32mtareas de resumen que requieren un an√°lisis m√°s amplio de un corpus completo. El enfoque m√°s adecuado es la \u001b[0m\n",
       "\u001b[32m'resumici√≥n enfocada en consultas' \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS, Dang, 2006\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, particularmente la 'resumici√≥n abstr√°ctica enfocada en \u001b[0m\n",
       "\u001b[32mconsultas', que genera res√∫menes en lenguaje natural y no solo extractos concatenados. La combinaci√≥n de m√©todos de\u001b[0m\n",
       "\u001b[32mQFS previos con RAG se sugiere como una soluci√≥n innovadora para mejorar la escalabilidad y efectividad en la \u001b[0m\n",
       "\u001b[32mrespuesta a preguntas sobre grandes vol√∫menes de texto no estructurado. El enfoque Graph RAG utiliza un LLM para \u001b[0m\n",
       "\u001b[32mconstruir un √≠ndice de texto basado en gr√°ficos en dos etapas, derivando primero un grafo de conocimiento de \u001b[0m\n",
       "\u001b[32mentidades y luego generando res√∫menes comunitarios para grupos de entidades relacionadas. La detecci√≥n de \u001b[0m\n",
       "\u001b[32mcomunidades \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Leiden, Traag et al., 2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m se utiliza para particionar el √≠ndice gr√°fico en grupos de elementos\u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mnodos, aristas, covariables\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que el LLM puede resumir en paralelo tanto en el tiempo de indexaci√≥n como en el de \u001b[0m\n",
       "\u001b[32mconsulta. Este m√©todo ha demostrado mejoras sustanciales en la exhaustividad y diversidad de las respuestas \u001b[0m\n",
       "\u001b[32mgeneradas en comparaci√≥n con un RAG ingenuo, especialmente para preguntas de sentido global sobre conjuntos de \u001b[0m\n",
       "\u001b[32mdatos de aproximadamente 1 mill√≥n de tokens. A pesar de los avances en la arquitectura transformadora, el desaf√≠o \u001b[0m\n",
       "\u001b[32mpersiste para la resumici√≥n abstr√°ctica enfocada en consultas sobre un corpus completo, ya que estos vol√∫menes de \u001b[0m\n",
       "\u001b[32mtexto pueden superar los l√≠mites de las ventanas de contexto de los LLM. Tales vol√∫menes de texto pueden superar \u001b[0m\n",
       "\u001b[32mampliamente los l√≠mites de las ventanas de contexto de los LLM, y la expansi√≥n de tales ventanas puede no ser \u001b[0m\n",
       "\u001b[32msuficiente dado que la informaci√≥n puede ser 'perdida en el medio' de contextos m√°s largos \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuratov et al., 2024; \u001b[0m\n",
       "\u001b[32mLiu et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Adem√°s, aunque la recuperaci√≥n directa de fragmentos de texto en RAG ingenuo probablemente sea \u001b[0m\n",
       "\u001b[32minadecuada para tareas de QFS, es posible que una forma alternativa de preindexaci√≥n pueda apoyar un nuevo enfoque \u001b[0m\n",
       "\u001b[32mRAG que apunte espec√≠ficamente a la resumici√≥n global. La 'respuesta global' a una consulta dada se produce \u001b[0m\n",
       "\u001b[32mmediante una ronda final de resumido enfocado en la consulta sobre todos los res√∫menes comunitarios que reportan \u001b[0m\n",
       "\u001b[32mrelevancia para esa consulta. El pipeline de Graph RAG utiliza un √≠ndice derivado de LLM que abarca nodos \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \u001b[0m\n",
       "\u001b[32mentidades\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, aristas \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., relaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m y covariables \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., afirmaciones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m que han sido detectadas, extra√≠das y \u001b[0m\n",
       "\u001b[32mresumidas mediante prompts de LLM adaptados al dominio del conjunto de datos. Se anticipa una implementaci√≥n de \u001b[0m\n",
       "\u001b[32mc√≥digo abierto en Python de los enfoques Graph RAG disponibles en https://aka.ms/graphrag. Se realizaron 20,000 \u001b[0m\n",
       "\u001b[32mextracciones y se detectaron 30,000 referencias de entidades, variando el tama√±o de fragmento \u001b[0m\u001b[32m(\u001b[0m\u001b[32mchunk size\u001b[0m\u001b[32m)\u001b[0m\u001b[32m a 600, \u001b[0m\n",
       "\u001b[32m1200 y 2400. Las descripciones comunitarias proporcionan una cobertura completa del √≠ndice gr√°fico subyacente y los\u001b[0m\n",
       "\u001b[32mdocumentos de entrada que representan. Los fragmentos de texto m√°s largos requieren menos llamadas al LLM para tal \u001b[0m\n",
       "\u001b[32mextracci√≥n, pero sufren de la degradaci√≥n de recuperaci√≥n de ventanas de contexto m√°s largas de LLM \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuratov et \u001b[0m\n",
       "\u001b[32mal., 2024; Liu et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Este comportamiento puede observarse en la Figura 2 en el caso de una √∫nica ronda de \u001b[0m\n",
       "\u001b[32mextracci√≥n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mes decir, con cero gleanings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: en un conjunto de datos de muestra \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHotPotQA, Yang et al., 2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mutilizando un tama√±o de fragmento de 600 tokens se extrajeron casi el doble de referencias de entidades que al \u001b[0m\n",
       "\u001b[32mutilizar un tama√±o de fragmento de 2400. Si bien m√°s referencias son generalmente mejores, cualquier proceso de \u001b[0m\n",
       "\u001b[32mextracci√≥n necesita equilibrar la recuperaci√≥n y la precisi√≥n para la actividad objetivo. La resumici√≥n enfocada en\u001b[0m\n",
       "\u001b[32mconsultas de un corpus completo se hace posible utilizando un enfoque de map-reduce: primero usando cada resumen \u001b[0m\n",
       "\u001b[32mcomunitario para responder a la consulta de manera independiente y en paralelo, luego resumiendo todas las \u001b[0m\n",
       "\u001b[32mrespuestas parciales relevantes en una respuesta global final. Para evaluar este enfoque, utilizamos un LLM para \u001b[0m\n",
       "\u001b[32mgenerar un conjunto diverso de preguntas\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(latest_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tKtoLf6DPv4Z",
   "metadata": {
    "id": "tKtoLf6DPv4Z"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5:** Synthetic Data Processing\n",
    "\n",
    "As we conclude our exploration of document summarization using LLMs, it's important to acknowledge the broader context and potential challenges. While we've demonstrated a viable method for extracting concise, meaningful summaries, let's consider why such an approach is crucial and the complexities it entails.\n",
    "\n",
    "#### **Generality of Refinement**\n",
    "\n",
    "It's important to note that this \"progressive summarization\" technique is merely a starter chain that makes few assumptions about the initial data and desired output format. The same technique can be expanded far and wide to generate synthetic refinements with known metadata, active assumptions, and downstream objectives in mind.\n",
    "\n",
    "**Consider these potential applications:**\n",
    "\n",
    "1. **Aggregating Data**: Constructing structures that transform raw data from document chunks into coherent, useful summaries.\n",
    "2. **Categorization and Sub-topic Analysis**: Creating systems that categorize insights from chunks into defined categories, tracking emerging sub-topics within each.\n",
    "3. **Consolidation into Dense Informational Chunks**: Refining these structures to distill insights into compact segments, enriched with direct quotes for deeper analysis.\n",
    "\n",
    "These applications hint at the creation of a **domain-specific knowledge graph** which can be accessed and traversed by a conversational chat model. Some utilities already exist to generate these automatically via tools like [**LangChain Knowledge Graphs**](https://python.langchain.com/docs/modules/memory/types/kg). Though you might need to develop hierarchical structures and tools to both construct and traverse such a structure, it is a viable option when you can properly refine a sufficient knowledge graph for your use case! For this intereted in more advanced knowledge graph construction techniques which rely on larger systems and vector similarity, we found the [**LangChain x Neo4j Article**](https://blog.langchain.dev/using-a-knowledge-graph-to-implement-a-devops-rag-application/) to be of interest.\n",
    "\n",
    "### **Addressing the Challenges of Large-Scale Data Processing**\n",
    "\n",
    "While our approach opens up exciting possibilities, it's not without its challenges, especially when dealing with large volumes of data:\n",
    "\n",
    "- **Generic Preprocessing Limitations**: While summarization is relatively straightforward, developing hierarchies that are universally effective across various contexts is challenging.\n",
    "\n",
    "- **Granularity and Navigation Costs**: Achieving detailed granularity in a hierarchy can be resource-intensive, requiring sophisticated consolidation or extensive branching to maintain manageable context sizes per interaction.\n",
    "\n",
    "- **Dependency on Precise Instruction Execution**: Navigating such a hierarchy with our current tools would rely heavily on powerful instruction-tuned models with strong prompt engineering. The inference latency and the risk of errors in argument prediction can be significant, so using LLMs for this could be a challenge.\n",
    "\n",
    "As you progress through the course, keep track of how these challenges get addressed with subsequent techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdFSMXOVRzEa",
   "metadata": {
    "id": "cdFSMXOVRzEa"
   },
   "source": [
    "-----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 6:** Wrap-Up\n",
    "\n",
    "The goal of this notebook was to introduce the problems and techniques surrounding large document handling for chat models. In the next notebook, we will investigate a complementary tool with a very different set of pros and cons; **semantic retrieval with embedding models.**\n",
    "\n",
    "### <font color=\"#76b900\">**Great Job!**</font>\n",
    "\n",
    "### **Next Steps:**\n",
    "1. **[Optional]** Revisit the **\"Questions To Think About\" Section** at the top of the notebook and think about some possible answers.\n",
    "2. **[Optional]** This notebooks includes some fundamental document processing chains, but does not touch upon [Map Reduce](https://python.langchain.com/docs/modules/chains/document/map_reduce) and [Map Rerank](https://python.langchain.com/docs/modules/chains/document/map_rerank) chains, which are also very useful but build on roughly the same intuitions. These are a good next step, so please check them out!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4",
   "metadata": {
    "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
